{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import copy\n",
    "import pickle\n",
    "import sympy\n",
    "import functools\n",
    "import itertools\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "from sklearn.decomposition import PCA\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.experimental import enable_iterative_imputer  \n",
    "from sklearn.impute import SimpleImputer, KNNImputer, IterativeImputer\n",
    "import copy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Rating Encoding: {'NR': 0, 'TV-14': 1, 'TV-G': 2, 'TV-MA': 3, 'TV-PG': 4, 'TV-Y': 5, 'TV-Y7': 6}\n",
      "Director Encoding: {'Alex Gibney': 0, 'Alexx Media': 1, 'Billy Corben': 2, 'Dan Forrer': 3, 'Daniel Minahan': 4, 'Danny Cannon': 5, 'David Schalko': 6, 'Eli Roth': 7, 'Eric Abrams': 8, 'Eric Goode, Rebecca Chaiklin': 9, 'Everardo Gout': 10, 'Glen Winter': 11, 'Glenn Weiss': 12, 'James Bamford': 13, 'Jared Hess, Tyler Measom': 14, 'Jay Chandrasekhar': 15, 'Jerry Seinfeld': 16, 'Jesse Moss': 17, 'Jesse Warn': 18, 'Jill Bauer, Ronna Gradus, Rashida Jones': 19, 'Joe Berlinger': 20, 'Joe Berlinger, Bruce Sinofsky': 21, 'Joel Gallen, Tig Notaro': 22, 'Joshua Zeman': 23, 'Julia Willoughby Nason, Jenner Furst': 24, 'Ken Burns': 25, 'Ken Burns, Lynn Novick': 26, 'Kenny Ortega': 27, 'Lee Toland Krieger': 28, 'Lynn Novick': 29, 'Marcus Raboy': 30, 'Michael Simon': 31, 'Oliver Stone': 32, 'Oscar Micheaux, Spencer Williams, Richard E. Norman, Richard Maurice': 33, 'Peter McDonnell': 34, 'Rob Seidenglanz': 35, 'Robert Kenner, Taki Oldham': 36, 'Ryan Polito': 37, 'Sharon Grimberg': 38, 'Stan Lathan': 39, 'Steven Bognar, Julia Reichert': 40, 'Thierry Demaizière, Alban Teurlai': 41, 'Tiller Russell': 42, 'Trey Borzillieri, Barbara Schroeder': 43, 'Vanessa Roth': 44, nan: 45}\n",
      "    director        country  rating  duration\n",
      "15        45  United States       3         4\n",
      "40        45  United States       6         1\n",
      "55        45  United States       4         6\n",
      "67        45  United States       4         9\n",
      "82        45  United States       1         6\n"
     ]
    }
   ],
   "source": [
    "# Load only relevant columns, TV shows from USA\n",
    "netflix_data = pd.read_csv('netflix_titles.csv', usecols=['type', 'director', 'country', 'rating', 'duration'])\n",
    "tv_shows = netflix_data[(netflix_data['type'] == 'TV Show') & (netflix_data['country'] == 'United States')].drop('type', axis=1)\n",
    "\n",
    "# Convert 'duration' to numeric values, handling \"Seasons\" separately\n",
    "tv_shows['duration'] = tv_shows['duration'].apply(lambda x: int(x.split()[0]) if 'Season' in x else None)\n",
    "tv_shows = tv_shows.dropna(subset=['duration'])\n",
    "\n",
    "# Initialize the LabelEncoder\n",
    "rating_encoder = LabelEncoder()\n",
    "director_encoder = LabelEncoder()\n",
    "tv_shows['rating'] = rating_encoder.fit_transform(tv_shows['rating'])\n",
    "tv_shows['director'] = director_encoder.fit_transform(tv_shows['director'])\n",
    "\n",
    "# Create dictionaries for original encodings\n",
    "rating_encoding = {original: encoded for original, encoded in zip(rating_encoder.classes_, range(len(rating_encoder.classes_)))}\n",
    "director_encoding = {original: encoded for original, encoded in zip(director_encoder.classes_, range(len(director_encoder.classes_)))}\n",
    "\n",
    "# Display the original encodings\n",
    "print(\"Rating Encoding:\", rating_encoding)\n",
    "print(\"Director Encoding:\", director_encoding)\n",
    "\n",
    "# Print the final DataFrame for verification (optional)\n",
    "print(tv_shows.head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_netflix(tv_shows):\n",
    "    # fetch dataset\n",
    "    features = [ 'rating', 'director']\n",
    "    X = tv_shows[features]\n",
    "    y = tv_shows['duration']\n",
    "\n",
    "    # with this random seed, no null value is included in the test data\n",
    "    X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=1)\n",
    "    X_train = copy.deepcopy(X_train).reset_index(drop=True)\n",
    "    X_test = copy.deepcopy(X_test).reset_index(drop=True)\n",
    "    y_train = y_train.reset_index(drop=True)\n",
    "    y_test = y_test.reset_index(drop=True)\n",
    "\n",
    "    return X_train, X_test, y_train, y_test\n",
    "\n",
    "# first impute the data and make it hypothetically clean\n",
    "def load_netflix_cleaned(tv_shows):\n",
    "    # fetch dataset\n",
    "    features = [ 'rating', 'director']\n",
    "    X = tv_shows[features]\n",
    "    y = tv_shows['duration']\n",
    "\n",
    "    # assumed gt imputation\n",
    "    imputer = KNNImputer(n_neighbors=10)\n",
    "    X = pd.DataFrame(imputer.fit_transform(X), columns=X.columns)\n",
    "\n",
    "    # Split the data into training and testing sets with a random seed\n",
    "    X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=1)\n",
    "    X_train = copy.deepcopy(X_train).reset_index(drop=True)\n",
    "    X_test = copy.deepcopy(X_test).reset_index(drop=True)\n",
    "    y_train = y_train.reset_index(drop=True)\n",
    "    y_test = y_test.reset_index(drop=True)\n",
    "\n",
    "    return X_train, X_test, y_train, y_test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train, X_test, y_train, y_test = load_netflix_cleaned()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>rating</th>\n",
       "      <th>director</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>5</td>\n",
       "      <td>45</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>3</td>\n",
       "      <td>45</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>5</td>\n",
       "      <td>45</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>1</td>\n",
       "      <td>45</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>1</td>\n",
       "      <td>45</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>603</th>\n",
       "      <td>3</td>\n",
       "      <td>45</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>604</th>\n",
       "      <td>1</td>\n",
       "      <td>45</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>605</th>\n",
       "      <td>3</td>\n",
       "      <td>45</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>606</th>\n",
       "      <td>6</td>\n",
       "      <td>45</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>607</th>\n",
       "      <td>3</td>\n",
       "      <td>45</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>608 rows × 2 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "     rating  director\n",
       "0         5        45\n",
       "1         3        45\n",
       "2         5        45\n",
       "3         1        45\n",
       "4         1        45\n",
       "..      ...       ...\n",
       "603       3        45\n",
       "604       1        45\n",
       "605       3        45\n",
       "606       6        45\n",
       "607       3        45\n",
       "\n",
       "[608 rows x 2 columns]"
      ]
     },
     "execution_count": 52,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# useful functions\n",
    "\n",
    "symbol_id = -1\n",
    "def create_symbol(suffix=''):\n",
    "    global symbol_id\n",
    "    symbol_id += 1\n",
    "    name = f'e{symbol_id}_{suffix}' if suffix else f'e{symbol_id}'\n",
    "    return sympy.Symbol(name=name)\n",
    "\n",
    "\n",
    "scaler_symbols = set([sb(f'k{i}') for i in range(X_train.shape[1]+1)])\n",
    "linearization_dict = dict()\n",
    "reverse_linearization_dict = dict()\n",
    "\n",
    "\n",
    "def sample_data(imputed_datasets, uncert_inds=[], seed=42):\n",
    "    imp_np = np.array(imputed_datasets)\n",
    "    if len(uncert_inds) == 0:\n",
    "        uncert_inds = list(itertools.product(range(imp_np.shape[1]),range(imp_np.shape[2])))\n",
    "    np.random.seed(seed)\n",
    "    choices = np.random.choice(np.arange(imp_np.shape[0]), len(uncert_inds), replace=True)\n",
    "    sample_result = imputed_datasets[0].copy()\n",
    "    for i, ind in enumerate(uncert_inds):\n",
    "        sample_result[ind[0]][ind[1]] = imputed_datasets[choices[i]][ind[0]][ind[1]]\n",
    "    return sample_result\n",
    "\n",
    "\n",
    "def linearization(expr_ls):\n",
    "    processed_expr_ls = [0 for _ in range(len(expr_ls))]\n",
    "    for expr_id, expr in enumerate(expr_ls):\n",
    "        # Do not support monomial expr currently, e.g., expr = 1.5*e1. \n",
    "        # At lease two monomials in expr, e.g., expr = 1.5*e1 + 2.\n",
    "        if not(expr.free_symbols):\n",
    "            processed_expr_ls[expr_id] += expr\n",
    "            continue\n",
    "        expr = expr.expand()\n",
    "        for arg in expr.args:\n",
    "            if not(arg.free_symbols):\n",
    "                processed_expr_ls[expr_id] += arg\n",
    "                continue\n",
    "            p = arg.as_poly()\n",
    "            monomial_exponents = p.monoms()[0]\n",
    "            \n",
    "            # only deal with non-linear monomials (order > 2)\n",
    "            if sum(monomial_exponents) <= 1:\n",
    "                processed_expr_ls[expr_id] += arg\n",
    "                continue\n",
    "\n",
    "            monomial = sympy.prod(x**k for x, k in zip(p.gens, monomial_exponents) \n",
    "                                  if not(x in scaler_symbols))\n",
    "            # check global substitution dictionary\n",
    "            if monomial in linearization_dict:\n",
    "                processed_expr_ls[expr_id] += arg.coeff(monomial)*linearization_dict[monomial]\n",
    "            else:\n",
    "                found = False\n",
    "                subs_monomial = create_symbol()\n",
    "                for symb in monomial.free_symbols:\n",
    "                    if symb in reverse_linearization_dict:\n",
    "                        equivalent_monomial = monomial.subs(symb, reverse_linearization_dict[symb])\n",
    "                        if equivalent_monomial in linearization_dict:\n",
    "                            subs_monomial = linearization_dict[equivalent_monomial]\n",
    "                            found = True\n",
    "                            break\n",
    "                linearization_dict[monomial] = subs_monomial\n",
    "                if not(found):\n",
    "                    reverse_linearization_dict[subs_monomial] = monomial\n",
    "                processed_expr_ls[expr_id] += arg.coeff(monomial)*subs_monomial\n",
    "                \n",
    "    return processed_expr_ls\n",
    "\n",
    "\n",
    "def merge_small_components_pca(expr_ls, budget=10):\n",
    "    if not(isinstance(expr_ls, sympy.Expr)):\n",
    "        expr_ls = sympy.Matrix(expr_ls)\n",
    "    if expr_ls.free_symbols:\n",
    "        center = expr_ls.subs(dict([(symb, 0) for symb in expr_ls.free_symbols]))\n",
    "    else:\n",
    "        return expr_ls\n",
    "    monomials_dict = get_generators(expr_ls)\n",
    "    generators = np.array([monomials_dict[m] for m in monomials_dict])\n",
    "    if len(generators) <= budget:\n",
    "        return expr_ls\n",
    "    monomials = [m for m in monomials_dict]\n",
    "    pca = PCA(n_components=len(generators[0]))\n",
    "    pca.fit(np.concatenate([generators, -generators]))\n",
    "    transformed_generators = pca.transform(generators)\n",
    "    transformed_generator_norms = np.linalg.norm(transformed_generators, axis=1, ord=2)\n",
    "    # from largest to lowest norm\n",
    "    sorted_indices = transformed_generator_norms.argsort()[::-1].astype(int)\n",
    "    sorted_transformed_generators = transformed_generators[sorted_indices]\n",
    "    sorted_monomials = [monomials[idx] for idx in sorted_indices]\n",
    "    new_transformed_generators = np.concatenate([sorted_transformed_generators[:budget], \n",
    "                                                 np.diag(np.sum(np.abs(sorted_transformed_generators[budget:]), \n",
    "                                                                axis=0))])\n",
    "    new_generators = pca.inverse_transform(new_transformed_generators)\n",
    "    new_monomials = sorted_monomials[:budget] + [create_symbol() for _ in range(len(generators[0]))]\n",
    "    \n",
    "    processed_expr_ls = center\n",
    "    for monomial_id in range(len(new_monomials)):\n",
    "        processed_expr_ls += sympy.Matrix(new_generators[monomial_id])*new_monomials[monomial_id]\n",
    "    \n",
    "    return processed_expr_ls\n",
    "\n",
    "\n",
    "def get_vertices(affset):\n",
    "    l = len(affset)\n",
    "    distinct_symbols = set()\n",
    "    for expr in affset:\n",
    "        if not(isinstance(expr, sympy.Expr)):\n",
    "            assert isinstance(expr, int) or isinstance(expr, float)\n",
    "        else:\n",
    "            if distinct_symbols:\n",
    "                distinct_symbols = distinct_symbols.union(expr.free_symbols)\n",
    "            else:\n",
    "                distinct_symbols = expr.free_symbols\n",
    "    distinct_symbols = list(distinct_symbols)\n",
    "    # print(distinct_symbols)\n",
    "    combs = [list(zip(distinct_symbols,list(l))) for l in list(itertools.product([-1, 1], repeat=len(distinct_symbols)))]\n",
    "    res = set()\n",
    "    for assignment in combs:\n",
    "        res.add(tuple([expr.subs(assignment) for expr in affset]))\n",
    "    return(res)\n",
    "\n",
    "\n",
    "# take a list of expressions as input, output the list of monomials and generator vectors,\n",
    "def get_generators(expr_ls):\n",
    "    monomials = dict()\n",
    "    for expr_id, expr in enumerate(expr_ls):\n",
    "        if not(isinstance(expr, sympy.Expr)) or not(expr.free_symbols):\n",
    "            continue\n",
    "        expr = expr.expand()\n",
    "        p = sympy.Poly(expr)\n",
    "        monomials_in_expr = [sympy.prod(x**k for x, k in zip(p.gens, mon)) \n",
    "                             for mon in p.monoms() if sum(mon) >= 1]\n",
    "        for monomial in monomials_in_expr:\n",
    "            coef = float(p.coeff_monomial(monomial))\n",
    "            if monomial in monomials:\n",
    "                if len(monomials[monomial]) < expr_id:\n",
    "                    monomials[monomial] = monomials[monomial] + [0 for _ in range(expr_id-len(monomials[monomial]))]\n",
    "                monomials[monomial].append(coef)\n",
    "            else:\n",
    "                monomials[monomial] = [0 for _ in range(expr_id)] + [coef]\n",
    "\n",
    "    for monomial in monomials:\n",
    "        if len(monomials[monomial]) < len(expr_ls):\n",
    "            monomials[monomial] = monomials[monomial] + [0 for _ in range(len(expr_ls)-len(monomials[monomial]))]\n",
    "    \n",
    "    return monomials\n",
    "\n",
    "\n",
    "def plot_conretiztion(affset, alpha = 0.5, color='red', budget=-1):\n",
    "    if budget > -1:\n",
    "        affset = merge_small_components_pca(affset, budget=budget)\n",
    "    pts = np.array(list(map(list, get_vertices(affset))))\n",
    "    hull = ConvexHull(pts)\n",
    "    plt.fill(pts[hull.vertices,0], pts[hull.vertices,1],color,alpha=alpha)\n",
    "    \n",
    "def inject_ranges(X, y, uncertain_attr, uncertain_num, uncertain_radius_pct=None, uncertain_radius=None, seed=42):\n",
    "    global symbol_id\n",
    "    symbol_id = -1\n",
    "    \n",
    "    X_extended = np.append(np.ones((len(X), 1)), X, axis=1)\n",
    "    ss = StandardScaler()\n",
    "    X_extended[:, 1:] = ss.fit_transform(X_extended[:, 1:])\n",
    "    X_extended_symb = sympy.Matrix(X_extended)\n",
    "    \n",
    "    if not(uncertain_attr=='y'):\n",
    "        uncertain_attr_idx = X.columns.to_list().index(uncertain_attr) + 1\n",
    "        if not(uncertain_radius):\n",
    "            uncertain_radius = uncertain_radius_pct*(np.max(X_extended[:, uncertain_attr_idx])-\\\n",
    "                                                     np.min(X_extended[:, uncertain_attr_idx]))\n",
    "    else:\n",
    "        if not(uncertain_radius):\n",
    "            uncertain_radius = uncertain_radius_pct*(y_train.max()-y_train.min())[0]\n",
    "    \n",
    "    np.random.seed(seed)\n",
    "    uncertain_indices = np.random.choice(range(len(y)), uncertain_num, replace=False)\n",
    "    y_symb = sympy.Matrix(y)\n",
    "    symbols_in_data = set()\n",
    "    for uncertain_idx in uncertain_indices:\n",
    "        new_symb = create_symbol()\n",
    "        symbols_in_data.add(new_symb)\n",
    "        if uncertain_attr=='y':\n",
    "            y_symb[uncertain_idx] = y_symb[uncertain_idx] + uncertain_radius*new_symb\n",
    "        else:\n",
    "            X_extended_symb[uncertain_idx, uncertain_attr_idx] = X_extended_symb[uncertain_idx, uncertain_attr_idx] + uncertain_radius*new_symb\n",
    "    return X_extended_symb, y_symb, symbols_in_data, ss\n",
    "\n",
    "\n",
    "def sample_data_from_ranges(X, y, seed=42):\n",
    "    all_free_symbols = X.free_symbols.union(y.free_symbols)\n",
    "    subs_dict = dict()\n",
    "    np.random.seed(seed)\n",
    "    for symb in all_free_symbols:\n",
    "        subs_dict[symb] = (np.random.uniform()-.5)*2\n",
    "    return X.subs(subs_dict), y.subs(subs_dict)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
