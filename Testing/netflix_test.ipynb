{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import time #Checking how long they run to measure"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import copy\n",
    "import pickle\n",
    "import sympy\n",
    "import functools\n",
    "import itertools\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "from error_injection import MissingValueError, SamplingError, Injector\n",
    "from sklearn.decomposition import PCA\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.experimental import enable_iterative_imputer\n",
    "from sklearn.impute import SimpleImputer, KNNImputer, IterativeImputer\n",
    "from sklearn.metrics import mutual_info_score, auc, roc_curve, roc_auc_score, f1_score\n",
    "from scipy.optimize import minimize as scipy_min\n",
    "from scipy.spatial import ConvexHull\n",
    "from scipy.optimize import minimize, Bounds, linprog\n",
    "from sympy import Symbol as sb\n",
    "from sympy import lambdify\n",
    "from tqdm.notebook import trange,tqdm\n",
    "from IPython.display import display,clear_output\n",
    "from random import choice\n",
    "\n",
    "class style():\n",
    "    RED = '\\033[31m'\n",
    "    GREEN = '\\033[32m'\n",
    "    BLUE = '\\033[34m'\n",
    "    RESET = '\\033[0m'\n",
    "\n",
    "np.random.seed(1)\n",
    "\n",
    "# ignore all the warnings\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Rating Encoding: {'TV-14': 0, 'TV-G': 1, 'TV-MA': 2, 'TV-PG': 3, 'TV-Y': 4, 'TV-Y7': 5}\n",
      "Director Encoding: {nan: 0}\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>director</th>\n",
       "      <th>country</th>\n",
       "      <th>rating</th>\n",
       "      <th>duration</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>15</th>\n",
       "      <td>0</td>\n",
       "      <td>United States</td>\n",
       "      <td>2</td>\n",
       "      <td>4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>40</th>\n",
       "      <td>0</td>\n",
       "      <td>United States</td>\n",
       "      <td>5</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>55</th>\n",
       "      <td>0</td>\n",
       "      <td>United States</td>\n",
       "      <td>3</td>\n",
       "      <td>6</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>67</th>\n",
       "      <td>0</td>\n",
       "      <td>United States</td>\n",
       "      <td>3</td>\n",
       "      <td>9</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>82</th>\n",
       "      <td>0</td>\n",
       "      <td>United States</td>\n",
       "      <td>0</td>\n",
       "      <td>6</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "    director        country  rating  duration\n",
       "15         0  United States       2         4\n",
       "40         0  United States       5         1\n",
       "55         0  United States       3         6\n",
       "67         0  United States       3         9\n",
       "82         0  United States       0         6"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Load only relevant columns, TV shows from USA\n",
    "netflix_data = pd.read_csv('netflix_titles.csv', usecols=['type', 'director', 'country', 'rating', 'duration'])\n",
    "tv_shows = netflix_data[(netflix_data['type'] == 'TV Show') & (netflix_data['country'] == 'United States')].drop('type', axis=1).head(20)\n",
    "\n",
    "# Convert 'duration' to numeric values, handling \"Seasons\" separately\n",
    "tv_shows['duration'] = tv_shows['duration'].apply(lambda x: int(x.split()[0]) if 'Season' in x else None)\n",
    "tv_shows = tv_shows.dropna(subset=['duration'])\n",
    "\n",
    "# Initialize the LabelEncoder\n",
    "rating_encoder = LabelEncoder()\n",
    "director_encoder = LabelEncoder()\n",
    "tv_shows['rating'] = rating_encoder.fit_transform(tv_shows['rating'])\n",
    "tv_shows['director'] = director_encoder.fit_transform(tv_shows['director'])\n",
    "\n",
    "# Create dictionaries for original encodings\n",
    "rating_encoding = {original: encoded for original, encoded in zip(rating_encoder.classes_, range(len(rating_encoder.classes_)))}\n",
    "director_encoding = {original: encoded for original, encoded in zip(director_encoder.classes_, range(len(director_encoder.classes_)))}\n",
    "\n",
    "# Display the original encodings\n",
    "print(\"Rating Encoding:\", rating_encoding)\n",
    "print(\"Director Encoding:\", director_encoding)\n",
    "\n",
    "# Print the final DataFrame for verification (optional)\n",
    "tv_shows.head()\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_netflix(tv_shows):\n",
    "    # fetch dataset\n",
    "    features = [ 'rating', 'director']\n",
    "    X = tv_shows[features]\n",
    "    y = tv_shows['duration']\n",
    "\n",
    "    # with this random seed, no null value is included in the test data\n",
    "    X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=1)\n",
    "    X_train = copy.deepcopy(X_train).reset_index(drop=True)\n",
    "    X_test = copy.deepcopy(X_test).reset_index(drop=True)\n",
    "    y_train = y_train.reset_index(drop=True)\n",
    "    y_test = y_test.reset_index(drop=True)\n",
    "\n",
    "    return X_train, X_test, y_train, y_test\n",
    "\n",
    "# first impute the data and make it hypothetically clean\n",
    "def load_netflix_cleaned(tv_shows):\n",
    "    # fetch dataset\n",
    "    features = [ 'rating', 'director']\n",
    "    X = tv_shows[features]\n",
    "    y = tv_shows['duration']\n",
    "\n",
    "    # assumed gt imputation\n",
    "    imputer = KNNImputer(n_neighbors=10)\n",
    "    X = pd.DataFrame(imputer.fit_transform(X), columns=X.columns)\n",
    "\n",
    "    # Split the data into training and testing sets with a random seed\n",
    "    X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=1)\n",
    "    X_train = copy.deepcopy(X_train).reset_index(drop=True)\n",
    "    X_test = copy.deepcopy(X_test).reset_index(drop=True)\n",
    "    y_train = y_train.reset_index(drop=True)\n",
    "    y_test = y_test.reset_index(drop=True)\n",
    "\n",
    "    return X_train, X_test, y_train, y_test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train, X_test, y_train, y_test = load_netflix_cleaned(tv_shows)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>rating</th>\n",
       "      <th>director</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>3.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>4.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>2.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>2.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>5.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>4.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>2.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>4.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>3.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>3.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>2.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>3.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15</th>\n",
       "      <td>5.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "    rating  director\n",
       "0      3.0       0.0\n",
       "1      4.0       0.0\n",
       "2      0.0       0.0\n",
       "3      2.0       0.0\n",
       "4      2.0       0.0\n",
       "5      5.0       0.0\n",
       "6      4.0       0.0\n",
       "7      2.0       0.0\n",
       "8      4.0       0.0\n",
       "9      3.0       0.0\n",
       "10     3.0       0.0\n",
       "11     2.0       0.0\n",
       "12     3.0       0.0\n",
       "13     0.0       0.0\n",
       "14     1.0       0.0\n",
       "15     5.0       0.0"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# useful functions\n",
    "\n",
    "symbol_id = -1\n",
    "def create_symbol(suffix=''):\n",
    "    global symbol_id\n",
    "    symbol_id += 1\n",
    "    name = f'e{symbol_id}_{suffix}' if suffix else f'e{symbol_id}'\n",
    "    return sympy.Symbol(name=name)\n",
    "\n",
    "\n",
    "scaler_symbols = set([sb(f'k{i}') for i in range(X_train.shape[1]+1)])\n",
    "linearization_dict = dict()\n",
    "reverse_linearization_dict = dict()\n",
    "\n",
    "\n",
    "def sample_data(imputed_datasets, uncert_inds=[], seed=42):\n",
    "    imp_np = np.array(imputed_datasets)\n",
    "    if len(uncert_inds) == 0:\n",
    "        uncert_inds = list(itertools.product(range(imp_np.shape[1]),range(imp_np.shape[2])))\n",
    "    np.random.seed(seed)\n",
    "    choices = np.random.choice(np.arange(imp_np.shape[0]), len(uncert_inds), replace=True)\n",
    "    sample_result = imputed_datasets[0].copy()\n",
    "    for i, ind in enumerate(uncert_inds):\n",
    "        sample_result[ind[0]][ind[1]] = imputed_datasets[choices[i]][ind[0]][ind[1]]\n",
    "    return sample_result\n",
    "\n",
    "\n",
    "def linearization(expr_ls):\n",
    "    processed_expr_ls = [0 for _ in range(len(expr_ls))]\n",
    "    for expr_id, expr in enumerate(expr_ls):\n",
    "        # Do not support monomial expr currently, e.g., expr = 1.5*e1. \n",
    "        # At lease two monomials in expr, e.g., expr = 1.5*e1 + 2.\n",
    "        if not(expr.free_symbols):\n",
    "            processed_expr_ls[expr_id] += expr\n",
    "            continue\n",
    "        expr = expr.expand()\n",
    "        for arg in expr.args:\n",
    "            if not(arg.free_symbols):\n",
    "                processed_expr_ls[expr_id] += arg\n",
    "                continue\n",
    "            p = arg.as_poly()\n",
    "            monomial_exponents = p.monoms()[0]\n",
    "            \n",
    "            # only deal with non-linear monomials (order > 2)\n",
    "            if sum(monomial_exponents) <= 1:\n",
    "                processed_expr_ls[expr_id] += arg\n",
    "                continue\n",
    "\n",
    "            monomial = sympy.prod(x**k for x, k in zip(p.gens, monomial_exponents) \n",
    "                                  if not(x in scaler_symbols))\n",
    "            # check global substitution dictionary\n",
    "            if monomial in linearization_dict:\n",
    "                processed_expr_ls[expr_id] += arg.coeff(monomial)*linearization_dict[monomial]\n",
    "            else:\n",
    "                found = False\n",
    "                subs_monomial = create_symbol()\n",
    "                for symb in monomial.free_symbols:\n",
    "                    if symb in reverse_linearization_dict:\n",
    "                        equivalent_monomial = monomial.subs(symb, reverse_linearization_dict[symb])\n",
    "                        if equivalent_monomial in linearization_dict:\n",
    "                            subs_monomial = linearization_dict[equivalent_monomial]\n",
    "                            found = True\n",
    "                            break\n",
    "                linearization_dict[monomial] = subs_monomial\n",
    "                if not(found):\n",
    "                    reverse_linearization_dict[subs_monomial] = monomial\n",
    "                processed_expr_ls[expr_id] += arg.coeff(monomial)*subs_monomial\n",
    "                \n",
    "    return processed_expr_ls\n",
    "\n",
    "\n",
    "def merge_small_components_pca(expr_ls, budget=10):\n",
    "    if not(isinstance(expr_ls, sympy.Expr)):\n",
    "        expr_ls = sympy.Matrix(expr_ls)\n",
    "    if expr_ls.free_symbols:\n",
    "        center = expr_ls.subs(dict([(symb, 0) for symb in expr_ls.free_symbols]))\n",
    "    else:\n",
    "        return expr_ls\n",
    "    monomials_dict = get_generators(expr_ls)\n",
    "    generators = np.array([monomials_dict[m] for m in monomials_dict])\n",
    "    if len(generators) <= budget:\n",
    "        return expr_ls\n",
    "    monomials = [m for m in monomials_dict]\n",
    "    pca = PCA(n_components=len(generators[0]))\n",
    "    pca.fit(np.concatenate([generators, -generators]))\n",
    "    transformed_generators = pca.transform(generators)\n",
    "    transformed_generator_norms = np.linalg.norm(transformed_generators, axis=1, ord=2)\n",
    "    # from largest to lowest norm\n",
    "    sorted_indices = transformed_generator_norms.argsort()[::-1].astype(int)\n",
    "    sorted_transformed_generators = transformed_generators[sorted_indices]\n",
    "    sorted_monomials = [monomials[idx] for idx in sorted_indices]\n",
    "    new_transformed_generators = np.concatenate([sorted_transformed_generators[:budget], \n",
    "                                                 np.diag(np.sum(np.abs(sorted_transformed_generators[budget:]), \n",
    "                                                                axis=0))])\n",
    "    new_generators = pca.inverse_transform(new_transformed_generators)\n",
    "    new_monomials = sorted_monomials[:budget] + [create_symbol() for _ in range(len(generators[0]))]\n",
    "    \n",
    "    processed_expr_ls = center\n",
    "    for monomial_id in range(len(new_monomials)):\n",
    "        processed_expr_ls += sympy.Matrix(new_generators[monomial_id])*new_monomials[monomial_id]\n",
    "    \n",
    "    return processed_expr_ls\n",
    "\n",
    "\n",
    "def get_vertices(affset):\n",
    "    l = len(affset)\n",
    "    distinct_symbols = set()\n",
    "    for expr in affset:\n",
    "        if not(isinstance(expr, sympy.Expr)):\n",
    "            assert isinstance(expr, int) or isinstance(expr, float)\n",
    "        else:\n",
    "            if distinct_symbols:\n",
    "                distinct_symbols = distinct_symbols.union(expr.free_symbols)\n",
    "            else:\n",
    "                distinct_symbols = expr.free_symbols\n",
    "    distinct_symbols = list(distinct_symbols)\n",
    "    # print(distinct_symbols)\n",
    "    combs = [list(zip(distinct_symbols,list(l))) for l in list(itertools.product([-1, 1], repeat=len(distinct_symbols)))]\n",
    "    res = set()\n",
    "    for assignment in combs:\n",
    "        res.add(tuple([expr.subs(assignment) for expr in affset]))\n",
    "    return(res)\n",
    "\n",
    "\n",
    "# take a list of expressions as input, output the list of monomials and generator vectors,\n",
    "def get_generators(expr_ls):\n",
    "    monomials = dict()\n",
    "    for expr_id, expr in enumerate(expr_ls):\n",
    "        if not(isinstance(expr, sympy.Expr)) or not(expr.free_symbols):\n",
    "            continue\n",
    "        expr = expr.expand()\n",
    "        p = sympy.Poly(expr)\n",
    "        monomials_in_expr = [sympy.prod(x**k for x, k in zip(p.gens, mon)) \n",
    "                             for mon in p.monoms() if sum(mon) >= 1]\n",
    "        for monomial in monomials_in_expr:\n",
    "            coef = float(p.coeff_monomial(monomial))\n",
    "            if monomial in monomials:\n",
    "                if len(monomials[monomial]) < expr_id:\n",
    "                    monomials[monomial] = monomials[monomial] + [0 for _ in range(expr_id-len(monomials[monomial]))]\n",
    "                monomials[monomial].append(coef)\n",
    "            else:\n",
    "                monomials[monomial] = [0 for _ in range(expr_id)] + [coef]\n",
    "\n",
    "    for monomial in monomials:\n",
    "        if len(monomials[monomial]) < len(expr_ls):\n",
    "            monomials[monomial] = monomials[monomial] + [0 for _ in range(len(expr_ls)-len(monomials[monomial]))]\n",
    "    \n",
    "    return monomials\n",
    "\n",
    "\n",
    "def plot_conretiztion(affset, alpha = 0.5, color='red', budget=-1):\n",
    "    if budget > -1:\n",
    "        affset = merge_small_components_pca(affset, budget=budget)\n",
    "    pts = np.array(list(map(list, get_vertices(affset))))\n",
    "    hull = ConvexHull(pts)\n",
    "    plt.fill(pts[hull.vertices,0], pts[hull.vertices,1],color,alpha=alpha)\n",
    "    \n",
    "def inject_ranges(X, y, uncertain_attr, uncertain_num, uncertain_radius_pct=None, uncertain_radius=None, seed=42):\n",
    "    global symbol_id\n",
    "    symbol_id = -1\n",
    "    \n",
    "    X_extended = np.append(np.ones((len(X), 1)), X, axis=1)\n",
    "    ss = StandardScaler()\n",
    "    X_extended[:, 1:] = ss.fit_transform(X_extended[:, 1:])\n",
    "    X_extended_symb = sympy.Matrix(X_extended)\n",
    "    \n",
    "    if not(uncertain_attr=='y'):\n",
    "        uncertain_attr_idx = X.columns.to_list().index(uncertain_attr) + 1\n",
    "        if not(uncertain_radius):\n",
    "            uncertain_radius = uncertain_radius_pct*(np.max(X_extended[:, uncertain_attr_idx])-\\\n",
    "                                                     np.min(X_extended[:, uncertain_attr_idx]))\n",
    "    else:\n",
    "        if not(uncertain_radius):\n",
    "            uncertain_radius = uncertain_radius_pct*(y_train.max()-y_train.min())[0]\n",
    "    \n",
    "    np.random.seed(seed)\n",
    "    uncertain_indices = np.random.choice(range(len(y)), uncertain_num, replace=False)\n",
    "    y_symb = sympy.Matrix(y)\n",
    "    symbols_in_data = set()\n",
    "    for uncertain_idx in uncertain_indices:\n",
    "        new_symb = create_symbol()\n",
    "        symbols_in_data.add(new_symb)\n",
    "        if uncertain_attr=='y':\n",
    "            y_symb[uncertain_idx] = y_symb[uncertain_idx] + uncertain_radius*new_symb\n",
    "        else:\n",
    "            X_extended_symb[uncertain_idx, uncertain_attr_idx] = X_extended_symb[uncertain_idx, uncertain_attr_idx] + uncertain_radius*new_symb\n",
    "    return X_extended_symb, y_symb, symbols_in_data, ss\n",
    "\n",
    "\n",
    "def sample_data_from_ranges(X, y, seed=42):\n",
    "    all_free_symbols = X.free_symbols.union(y.free_symbols)\n",
    "    subs_dict = dict()\n",
    "    np.random.seed(seed)\n",
    "    for symb in all_free_symbols:\n",
    "        subs_dict[symb] = (np.random.uniform()-.5)*2\n",
    "    return X.subs(subs_dict), y.subs(subs_dict)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Verification Test Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_robustness_ratio(X_train, y_train, X_test, y_test, robustness_radius, uncertain_attr, \n",
    "                             uncertain_num, uncertain_radius=None, uncertain_radius_ratio=None, \n",
    "                             lr=0.1, seed=42):\n",
    "    X, y, symbols_in_data, ss = inject_ranges(X=X_train, y=y_train, uncertain_attr=uncertain_attr, \n",
    "                                              uncertain_num=uncertain_num, uncertain_radius=uncertain_radius, \n",
    "                                              uncertain_radius_pct=uncertain_radius_ratio, seed=seed)\n",
    "    \n",
    "    n = X.shape[0]\n",
    "    XS = copy.deepcopy(X)\n",
    "    XR = copy.deepcopy(X)\n",
    "    yS = copy.deepcopy(y)\n",
    "    yR = copy.deepcopy(y)\n",
    "\n",
    "    for row in range(X.shape[0]):\n",
    "        for col in range(X.shape[1]):\n",
    "            expr = X[row, col]\n",
    "            if isinstance(expr, sympy.Expr) and expr.free_symbols:\n",
    "                XR[row, col] = expr.subs(dict([(symb, 0) for symb in expr.free_symbols]))\n",
    "                XS[row, col] = expr - XR[row, col]\n",
    "            else:\n",
    "                XR[row, col] = expr\n",
    "                XS[row, col] = 0\n",
    "\n",
    "    for row in range(y.shape[0]):\n",
    "        expr = y[row]\n",
    "        if isinstance(expr, sympy.Expr) and expr.free_symbols:\n",
    "            yR[row] = expr.subs(dict([(symb, 0) for symb in expr.free_symbols]))\n",
    "            yS[row] = expr - yR[row]\n",
    "        else:\n",
    "            yR[row] = expr\n",
    "            yS[row] = 0\n",
    "\n",
    "    VT, sigma, V = np.linalg.svd(np.array((XR.T*XR).tolist()).astype(float))\n",
    "    V = sympy.Matrix(V)\n",
    "\n",
    "    wR = (XR.T*XR).inv()*XR.T*yR\n",
    "    wS_non_data = 0.0*V.row(0).T\n",
    "    for i in range(X.shape[1]):\n",
    "        wS_non_data = wS_non_data + sb(f'k{i}')*sb(f'ep{i}')*V.row(i).T\n",
    "\n",
    "    eigenvalues = 1 - sigma/n*2*lr\n",
    "    for eigenvalue in eigenvalues:\n",
    "        try:\n",
    "            assert abs(eigenvalue) <= 1\n",
    "            assert eigenvalue >= 0\n",
    "        except:\n",
    "            print(X)\n",
    "            raise ArithmeticError('Inappropriate lambda!')\n",
    "    \n",
    "    A = V.T*np.diag(eigenvalues)*V\n",
    "\n",
    "    wS_data = (np.identity(X.shape[1])-A).inv()*((XS.T*XR + XR.T*XS)*wR - XS.T*yR - XR.T*yS)*(-lr*2/n)\n",
    "\n",
    "    wS = wS_non_data + wS_data\n",
    "    w = wS + wR\n",
    "    w_prime = (-lr*2/n)*((XS.T*XR + XR.T*XS + XS.T*XS)*wS + XS.T*XS*wR - XS.T*yS).expand()\n",
    "    w_prime_projected = V*w_prime\n",
    "    \n",
    "    eqs = []\n",
    "    for d in range(X.shape[1]):\n",
    "        eq1 = (1-abs(eigenvalues[d]))*sb(f'k{d}')\n",
    "        eq2 = 0\n",
    "        coef_dict = dict()\n",
    "        coef_dict['const'] = 0\n",
    "        for i in range(X.shape[1]):\n",
    "            coef_dict[sb(f'k{i}')] = 0\n",
    "        for arg in w_prime_projected[d].args:\n",
    "            contain_k = False\n",
    "            for i in range(X.shape[1]):\n",
    "                symb_k = sb(f'k{i}')\n",
    "                if symb_k in arg.free_symbols:\n",
    "                    coef_dict[symb_k] = coef_dict[symb_k] + abs(arg.args[0])\n",
    "                    contain_k = True\n",
    "                    break\n",
    "            if not(contain_k):\n",
    "                coef_dict['const'] = coef_dict['const'] + abs(arg.args[0])\n",
    "        eq2 = coef_dict['const']\n",
    "        for i in range(X.shape[1]):\n",
    "            eq2 = eq2 + sb(f'k{i}')*coef_dict[sb(f'k{i}')]\n",
    "        eqs.append(sympy.Eq(eq1, eq2))\n",
    "        \n",
    "    result = sympy.solve(eqs, [sb(f'k{i}') for i in range(X.shape[1])])\n",
    "    for ki in result:\n",
    "        try:\n",
    "            assert result[ki] >= 0\n",
    "        except:\n",
    "            print(result)\n",
    "            print(eqs)\n",
    "            raise ArithmeticError('Negative k!')\n",
    "    param = wR + wS.subs(result)\n",
    "    \n",
    "    test_preds = sympy.Matrix(np.append(np.ones((len(X_test), 1)), ss.transform(X_test), axis=1))*param\n",
    "    robustness_ls = []\n",
    "    for pred in test_preds:\n",
    "        pred_range_radius = 0\n",
    "        for arg in pred.args:\n",
    "            if arg.free_symbols:\n",
    "                pred_range_radius += abs(arg.args[0])\n",
    "        if pred_range_radius <= robustness_radius:\n",
    "            robustness_ls.append(1)\n",
    "        else:\n",
    "            robustness_ls.append(0)\n",
    "    \n",
    "#     print(param)\n",
    "    return np.mean(robustness_ls)\n",
    "\n",
    "\n",
    "# if interval=True, use interval arithmetic, otherwise use zonotopes\n",
    "def compute_robustness_ratio_label_error(X_train, y_train, X_test, y_test, robustness_radius,\n",
    "                                         uncertain_num, uncertain_radius=None, \n",
    "                                         lr=0.1, seed=42, interval=True):\n",
    "    X, y, symbols_in_data, ss = inject_ranges(X=X_train, y=y_train, uncertain_attr='y', \n",
    "                                              uncertain_num=uncertain_num, uncertain_radius=uncertain_radius, \n",
    "                                              uncertain_radius_pct=None, seed=seed)\n",
    "    \n",
    "    assert len(X.free_symbols)==0\n",
    "    # closed-form\n",
    "    param = (X.T*X).inv()*X.T*y\n",
    "    \n",
    "    if interval:\n",
    "        # make param intervals\n",
    "        for d in range(len(param)):\n",
    "            expr = param[d]\n",
    "            if not(expr.free_symbols):\n",
    "                continue\n",
    "            else:\n",
    "                constant_part = 0\n",
    "                interval_radius = 0\n",
    "                for arg in expr.args:\n",
    "                    if arg.free_symbols:\n",
    "                        interval_radius += abs(arg.args[0])\n",
    "                    else:\n",
    "                        assert constant_part == 0\n",
    "                        constant_part = arg\n",
    "                param[d] = constant_part + create_symbol()*interval_radius\n",
    "    \n",
    "    test_preds = sympy.Matrix(np.append(np.ones((len(X_test), 1)), ss.transform(X_test), axis=1))*param\n",
    "    robustness_ls = []\n",
    "    for pred in test_preds:\n",
    "        pred_range_radius = 0\n",
    "        for arg in pred.args:\n",
    "            if arg.free_symbols:\n",
    "                pred_range_radius += abs(arg.args[0])\n",
    "        if pred_range_radius <= robustness_radius:\n",
    "            robustness_ls.append(1)\n",
    "        else:\n",
    "            robustness_ls.append(0)\n",
    "    \n",
    "#     print(param)\n",
    "    return np.mean(robustness_ls)\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Zonotope + Label Error test**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "start_time = time.time()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "robustness_dicts = []\n",
    "for seed in tqdm(range(5), desc=f'Progress'):\n",
    "    robustness_radius = 500\n",
    "    label_range = (y_train.max()-y_train.min())\n",
    "    ratios = [0.02, 0.04, 0.06, 0.08]\n",
    "    uncertain_radiuses = [ratio*label_range for ratio in ratios]\n",
    "    uncertain_pcts = list(np.arange(1, 11)/100)\n",
    "    robustness_dict = dict()\n",
    "    robustness_dict['uncertain_radius'] = uncertain_radiuses\n",
    "    robustness_dict['uncertain_radius_ratios'] = ratios\n",
    "    for uncertain_pct in tqdm(uncertain_pcts, desc=f'Rep {seed+1}', leave=False):\n",
    "        robustness_dict[uncertain_pct] = list()\n",
    "        uncertain_num = int(uncertain_pct*len(y_train))\n",
    "        for uncertain_radius in tqdm(uncertain_radiuses, desc=f'Varying Uncertain Radius', leave=False):\n",
    "            robustness_ratio = compute_robustness_ratio_label_error(X_train, y_train, X_test, y_test, \n",
    "                                                                    uncertain_num=uncertain_num, \n",
    "                                                                    uncertain_radius=uncertain_radius, \n",
    "                                                                    robustness_radius=robustness_radius, \n",
    "                                                                    interval=False, seed=seed)\n",
    "            robustness_dict[uncertain_pct].append(robustness_ratio)\n",
    "    robustness_dicts.append(robustness_dict)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "end_time = time.time()\n",
    "\n",
    "execution_time = end_time - start_time\n",
    "print(f\"Execution time: {execution_time} seconds\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Interval + Label Error test**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "start_time = time.time()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "robustness_dicts_interval = []\n",
    "for seed in tqdm(range(5), desc=f'Progress'):\n",
    "    # mpg +- 2 is robust\n",
    "    robustness_radius = 500\n",
    "    label_range = (y_train.max()-y_train.min())\n",
    "    ratios = [0.02, 0.04, 0.06, 0.08]\n",
    "    uncertain_radiuses = [ratio*label_range for ratio in ratios]\n",
    "    uncertain_pcts = list(np.arange(1, 11)/100)\n",
    "    robustness_dict_interval = dict()\n",
    "    robustness_dict_interval['uncertain_radius'] = uncertain_radiuses\n",
    "    robustness_dict_interval['uncertain_radius_ratios'] = ratios\n",
    "    for uncertain_pct in tqdm(uncertain_pcts, desc=f'Rep {seed+1}', leave=False):\n",
    "        robustness_dict_interval[uncertain_pct] = list()\n",
    "        uncertain_num = int(uncertain_pct*len(y_train))\n",
    "        for uncertain_radius in tqdm(uncertain_radiuses, desc=f'Varying Uncertain Radius', leave=False):\n",
    "            robustness_ratio = compute_robustness_ratio_label_error(X_train, y_train, X_test, y_test, \n",
    "                                                                    uncertain_num=uncertain_num, \n",
    "                                                                    uncertain_radius=uncertain_radius, \n",
    "                                                                    robustness_radius=robustness_radius, \n",
    "                                                                    interval=True, seed=seed)\n",
    "            robustness_dict_interval[uncertain_pct].append(robustness_ratio)\n",
    "    robustness_dicts_interval.append(robustness_dict_interval)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "end_time = time.time()\n",
    "\n",
    "execution_time = end_time - start_time\n",
    "print(f\"Execution time: {execution_time} seconds\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "robustness_interval_mean = sum([pd.DataFrame(robustness_dicts_interval[i]).iloc[:, 2:] for i in range(5)])/5\n",
    "robustness_interval_std = (sum([(pd.DataFrame(robustness_dicts_interval[i]).iloc[:, 2:]-robustness_interval_mean)**2 for i in range(5)])/5).apply(np.sqrt)\n",
    "robustness_zonotope_mean = sum([pd.DataFrame(robustness_dicts[i]).iloc[:, 2:] for i in range(5)])/5\n",
    "robustness_zonotope_std = (sum([(pd.DataFrame(robustness_dicts[i]).iloc[:, 2:]-robustness_zonotope_mean)**2 for i in range(5)])/5).apply(np.sqrt)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## HEATMAP PLOT"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
