{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "b7cd733b",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.datasets import load_breast_cancer\n",
    "from sklearn import model_selection\n",
    "from sklearn import linear_model\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import random\n",
    "from imblearn.under_sampling import RandomUnderSampler\n",
    "from imblearn.pipeline import Pipeline\n",
    "np.random.seed(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "bc833c77",
   "metadata": {},
   "outputs": [],
   "source": [
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e17d0423",
   "metadata": {},
   "source": [
    "The goal here is to showcase the advantages of data augmentation and generation on\n",
    "dataset models with issues that prevent them from being ideal for training prediction models\n",
    "on. Consider the below set:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "4c2c5621",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>year</th>\n",
       "      <th>A1</th>\n",
       "      <th>A2</th>\n",
       "      <th>A3</th>\n",
       "      <th>A4</th>\n",
       "      <th>A5</th>\n",
       "      <th>A6</th>\n",
       "      <th>A7</th>\n",
       "      <th>A8</th>\n",
       "      <th>A9</th>\n",
       "      <th>...</th>\n",
       "      <th>A55</th>\n",
       "      <th>A56</th>\n",
       "      <th>A57</th>\n",
       "      <th>A58</th>\n",
       "      <th>A59</th>\n",
       "      <th>A60</th>\n",
       "      <th>A61</th>\n",
       "      <th>A62</th>\n",
       "      <th>A63</th>\n",
       "      <th>A64</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1</td>\n",
       "      <td>0.200550</td>\n",
       "      <td>0.37951</td>\n",
       "      <td>0.396410</td>\n",
       "      <td>2.0472</td>\n",
       "      <td>32.351</td>\n",
       "      <td>0.388250</td>\n",
       "      <td>0.249760</td>\n",
       "      <td>1.33050</td>\n",
       "      <td>1.1389</td>\n",
       "      <td>...</td>\n",
       "      <td>348690.0000</td>\n",
       "      <td>0.121960</td>\n",
       "      <td>0.397180</td>\n",
       "      <td>0.87804</td>\n",
       "      <td>0.001924</td>\n",
       "      <td>8.4160</td>\n",
       "      <td>5.1372</td>\n",
       "      <td>82.658</td>\n",
       "      <td>4.4158</td>\n",
       "      <td>7.42770</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>1</td>\n",
       "      <td>0.009020</td>\n",
       "      <td>0.63202</td>\n",
       "      <td>0.053735</td>\n",
       "      <td>1.1263</td>\n",
       "      <td>-37.842</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.014434</td>\n",
       "      <td>0.58223</td>\n",
       "      <td>1.3332</td>\n",
       "      <td>...</td>\n",
       "      <td>1.1263</td>\n",
       "      <td>0.180110</td>\n",
       "      <td>0.024512</td>\n",
       "      <td>0.84165</td>\n",
       "      <td>0.340940</td>\n",
       "      <td>9.9665</td>\n",
       "      <td>4.2382</td>\n",
       "      <td>116.500</td>\n",
       "      <td>3.1330</td>\n",
       "      <td>2.56030</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>1</td>\n",
       "      <td>0.266690</td>\n",
       "      <td>0.34994</td>\n",
       "      <td>0.611470</td>\n",
       "      <td>3.0243</td>\n",
       "      <td>43.087</td>\n",
       "      <td>0.559830</td>\n",
       "      <td>0.332070</td>\n",
       "      <td>1.85770</td>\n",
       "      <td>1.1268</td>\n",
       "      <td>...</td>\n",
       "      <td>5340.0000</td>\n",
       "      <td>0.112500</td>\n",
       "      <td>0.410250</td>\n",
       "      <td>0.88750</td>\n",
       "      <td>0.073630</td>\n",
       "      <td>9.5593</td>\n",
       "      <td>5.6298</td>\n",
       "      <td>38.168</td>\n",
       "      <td>9.5629</td>\n",
       "      <td>33.41300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>1</td>\n",
       "      <td>0.067731</td>\n",
       "      <td>0.19885</td>\n",
       "      <td>0.081562</td>\n",
       "      <td>2.9576</td>\n",
       "      <td>90.606</td>\n",
       "      <td>0.212650</td>\n",
       "      <td>0.078063</td>\n",
       "      <td>4.02900</td>\n",
       "      <td>1.2570</td>\n",
       "      <td>...</td>\n",
       "      <td>15132.0000</td>\n",
       "      <td>0.204440</td>\n",
       "      <td>0.084542</td>\n",
       "      <td>0.79556</td>\n",
       "      <td>0.196190</td>\n",
       "      <td>8.2122</td>\n",
       "      <td>2.7917</td>\n",
       "      <td>60.218</td>\n",
       "      <td>6.0613</td>\n",
       "      <td>0.28803</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>1</td>\n",
       "      <td>-0.029182</td>\n",
       "      <td>0.21131</td>\n",
       "      <td>0.452640</td>\n",
       "      <td>7.5746</td>\n",
       "      <td>57.844</td>\n",
       "      <td>0.010387</td>\n",
       "      <td>-0.034653</td>\n",
       "      <td>3.73240</td>\n",
       "      <td>1.0241</td>\n",
       "      <td>...</td>\n",
       "      <td>34549.0000</td>\n",
       "      <td>0.023565</td>\n",
       "      <td>-0.037001</td>\n",
       "      <td>0.97644</td>\n",
       "      <td>0.180630</td>\n",
       "      <td>3.4646</td>\n",
       "      <td>11.3380</td>\n",
       "      <td>31.807</td>\n",
       "      <td>11.4750</td>\n",
       "      <td>1.65110</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows × 65 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "    year        A1       A2        A3      A4      A5        A6        A7  \\\n",
       "0      1  0.200550  0.37951  0.396410  2.0472  32.351  0.388250  0.249760   \n",
       "8      1  0.009020  0.63202  0.053735  1.1263 -37.842  0.000000  0.014434   \n",
       "12     1  0.266690  0.34994  0.611470  3.0243  43.087  0.559830  0.332070   \n",
       "13     1  0.067731  0.19885  0.081562  2.9576  90.606  0.212650  0.078063   \n",
       "14     1 -0.029182  0.21131  0.452640  7.5746  57.844  0.010387 -0.034653   \n",
       "\n",
       "         A8      A9  ...          A55       A56       A57      A58       A59  \\\n",
       "0   1.33050  1.1389  ...  348690.0000  0.121960  0.397180  0.87804  0.001924   \n",
       "8   0.58223  1.3332  ...       1.1263  0.180110  0.024512  0.84165  0.340940   \n",
       "12  1.85770  1.1268  ...    5340.0000  0.112500  0.410250  0.88750  0.073630   \n",
       "13  4.02900  1.2570  ...   15132.0000  0.204440  0.084542  0.79556  0.196190   \n",
       "14  3.73240  1.0241  ...   34549.0000  0.023565 -0.037001  0.97644  0.180630   \n",
       "\n",
       "       A60      A61      A62      A63       A64  \n",
       "0   8.4160   5.1372   82.658   4.4158   7.42770  \n",
       "8   9.9665   4.2382  116.500   3.1330   2.56030  \n",
       "12  9.5593   5.6298   38.168   9.5629  33.41300  \n",
       "13  8.2122   2.7917   60.218   6.0613   0.28803  \n",
       "14  3.4646  11.3380   31.807  11.4750   1.65110  \n",
       "\n",
       "[5 rows x 65 columns]"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from ucimlrepo import fetch_ucirepo\n",
    "\n",
    "polish_companies_bankruptcy = fetch_ucirepo(id=365)\n",
    " \n",
    "# note we drop the missing values for ease of predictions down the line towards proving improvements with\n",
    "# data augmentation. In a more thorough study, we would do something to keep these rows\n",
    "df = pd.DataFrame(polish_companies_bankruptcy.data.features).dropna()\n",
    "\n",
    "targets = polish_companies_bankruptcy.data.targets[\"class\"]\n",
    "targets = targets[df.index]\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1512306c",
   "metadata": {},
   "source": [
    "Above is a dataset based off of Polish Banks. The features aren't labeled here (for example the first 2 are \"net profit/total assests\" and \"total liabilities / total assets\") but they're largely irrelevant to know here, just realize they have some weight to the prediction we're aiming for. In this case, we're trying to predict banks that are likely to go bankrupt where (1) is positive and (0) is negative. Let's get some statistics on the dataset we have here, along with includ"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "d46f8bbb",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "19967"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(df) #number of rows"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "c0f2a1f9",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "class\n",
       "0    19535\n",
       "1      432\n",
       "Name: count, dtype: int64"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "targets.value_counts() #number of bankruptcy(1) and non-bankruptcy(0) values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "8984337b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "class\n",
       "0    0.978364\n",
       "1    0.021636\n",
       "Name: count, dtype: float64"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#percentage of the above values. Notice how greatly imbalanced the two targets are.\n",
    "targets.value_counts()/len(targets) "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e2fe2af0",
   "metadata": {},
   "source": [
    "The imbalance between the labels will prove to be an issue in predictions. Now let's split the data set as follows:\n",
    "\n",
    "- ~ 20% testing\n",
    "- ~ 80% training\n",
    "    - ~ 80% training\n",
    "    - ~ 20% validation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "f17951da",
   "metadata": {},
   "outputs": [],
   "source": [
    "X = df\n",
    "y = targets\n",
    "X_train, X_test, y_train, y_test = model_selection.train_test_split(X, y, test_size=0.2)\n",
    "X_train, X_valid, y_train, y_valid = model_selection.train_test_split(X_train, y_train, test_size=0.2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "079079e3",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>year</th>\n",
       "      <th>A1</th>\n",
       "      <th>A2</th>\n",
       "      <th>A3</th>\n",
       "      <th>A4</th>\n",
       "      <th>A5</th>\n",
       "      <th>A6</th>\n",
       "      <th>A7</th>\n",
       "      <th>A8</th>\n",
       "      <th>A9</th>\n",
       "      <th>...</th>\n",
       "      <th>A55</th>\n",
       "      <th>A56</th>\n",
       "      <th>A57</th>\n",
       "      <th>A58</th>\n",
       "      <th>A59</th>\n",
       "      <th>A60</th>\n",
       "      <th>A61</th>\n",
       "      <th>A62</th>\n",
       "      <th>A63</th>\n",
       "      <th>A64</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>21633</th>\n",
       "      <td>3</td>\n",
       "      <td>0.010548</td>\n",
       "      <td>0.28097</td>\n",
       "      <td>0.19354</td>\n",
       "      <td>1.8976</td>\n",
       "      <td>41.0800</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.010548</td>\n",
       "      <td>2.55900</td>\n",
       "      <td>1.0244</td>\n",
       "      <td>...</td>\n",
       "      <td>650.71</td>\n",
       "      <td>0.036687</td>\n",
       "      <td>0.01467</td>\n",
       "      <td>0.98972</td>\n",
       "      <td>0.079423</td>\n",
       "      <td>13.030</td>\n",
       "      <td>4.4091</td>\n",
       "      <td>76.826</td>\n",
       "      <td>4.7510</td>\n",
       "      <td>1.7338</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>606</th>\n",
       "      <td>1</td>\n",
       "      <td>0.056155</td>\n",
       "      <td>0.53111</td>\n",
       "      <td>0.31580</td>\n",
       "      <td>2.0809</td>\n",
       "      <td>32.4850</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.069669</td>\n",
       "      <td>0.88283</td>\n",
       "      <td>1.7005</td>\n",
       "      <td>...</td>\n",
       "      <td>11168.00</td>\n",
       "      <td>0.040438</td>\n",
       "      <td>0.11976</td>\n",
       "      <td>0.95962</td>\n",
       "      <td>0.084484</td>\n",
       "      <td>10.198</td>\n",
       "      <td>13.4680</td>\n",
       "      <td>62.712</td>\n",
       "      <td>5.8202</td>\n",
       "      <td>4.3379</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3076</th>\n",
       "      <td>1</td>\n",
       "      <td>0.190690</td>\n",
       "      <td>0.47717</td>\n",
       "      <td>0.15383</td>\n",
       "      <td>1.7175</td>\n",
       "      <td>3.0048</td>\n",
       "      <td>0.019519</td>\n",
       "      <td>0.238900</td>\n",
       "      <td>1.09570</td>\n",
       "      <td>1.6844</td>\n",
       "      <td>...</td>\n",
       "      <td>824.19</td>\n",
       "      <td>0.153740</td>\n",
       "      <td>0.36472</td>\n",
       "      <td>0.85820</td>\n",
       "      <td>0.470660</td>\n",
       "      <td>12.032</td>\n",
       "      <td>8.0748</td>\n",
       "      <td>46.458</td>\n",
       "      <td>7.8565</td>\n",
       "      <td>2.6661</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12160</th>\n",
       "      <td>2</td>\n",
       "      <td>0.049596</td>\n",
       "      <td>0.39135</td>\n",
       "      <td>0.14268</td>\n",
       "      <td>1.4014</td>\n",
       "      <td>-13.5750</td>\n",
       "      <td>0.149020</td>\n",
       "      <td>0.059049</td>\n",
       "      <td>1.24420</td>\n",
       "      <td>1.0357</td>\n",
       "      <td>...</td>\n",
       "      <td>6787.20</td>\n",
       "      <td>0.034483</td>\n",
       "      <td>0.10186</td>\n",
       "      <td>0.96552</td>\n",
       "      <td>0.073668</td>\n",
       "      <td>10.094</td>\n",
       "      <td>8.1199</td>\n",
       "      <td>61.666</td>\n",
       "      <td>5.9190</td>\n",
       "      <td>4.1928</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>34384</th>\n",
       "      <td>4</td>\n",
       "      <td>0.068193</td>\n",
       "      <td>0.34917</td>\n",
       "      <td>0.23161</td>\n",
       "      <td>1.7112</td>\n",
       "      <td>-28.7140</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.068193</td>\n",
       "      <td>1.86390</td>\n",
       "      <td>1.1223</td>\n",
       "      <td>...</td>\n",
       "      <td>1375.40</td>\n",
       "      <td>-0.110310</td>\n",
       "      <td>0.10478</td>\n",
       "      <td>1.17090</td>\n",
       "      <td>0.011255</td>\n",
       "      <td>4.712</td>\n",
       "      <td>3.5793</td>\n",
       "      <td>105.920</td>\n",
       "      <td>3.4460</td>\n",
       "      <td>2.5351</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows × 65 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "       year        A1       A2       A3      A4       A5        A6        A7  \\\n",
       "21633     3  0.010548  0.28097  0.19354  1.8976  41.0800  0.000000  0.010548   \n",
       "606       1  0.056155  0.53111  0.31580  2.0809  32.4850  0.000000  0.069669   \n",
       "3076      1  0.190690  0.47717  0.15383  1.7175   3.0048  0.019519  0.238900   \n",
       "12160     2  0.049596  0.39135  0.14268  1.4014 -13.5750  0.149020  0.059049   \n",
       "34384     4  0.068193  0.34917  0.23161  1.7112 -28.7140  0.000000  0.068193   \n",
       "\n",
       "            A8      A9  ...       A55       A56      A57      A58       A59  \\\n",
       "21633  2.55900  1.0244  ...    650.71  0.036687  0.01467  0.98972  0.079423   \n",
       "606    0.88283  1.7005  ...  11168.00  0.040438  0.11976  0.95962  0.084484   \n",
       "3076   1.09570  1.6844  ...    824.19  0.153740  0.36472  0.85820  0.470660   \n",
       "12160  1.24420  1.0357  ...   6787.20  0.034483  0.10186  0.96552  0.073668   \n",
       "34384  1.86390  1.1223  ...   1375.40 -0.110310  0.10478  1.17090  0.011255   \n",
       "\n",
       "          A60      A61      A62     A63     A64  \n",
       "21633  13.030   4.4091   76.826  4.7510  1.7338  \n",
       "606    10.198  13.4680   62.712  5.8202  4.3379  \n",
       "3076   12.032   8.0748   46.458  7.8565  2.6661  \n",
       "12160  10.094   8.1199   61.666  5.9190  4.1928  \n",
       "34384   4.712   3.5793  105.920  3.4460  2.5351  \n",
       "\n",
       "[5 rows x 65 columns]"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_train.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "68b664a5",
   "metadata": {},
   "source": [
    "We'll use the accuracy, precision, recall, and the f1-score to determine the performance of each model. Our reason will be listed below when determining the base model performance without any data augmentation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "9b966c07",
   "metadata": {},
   "outputs": [],
   "source": [
    "#accuracy score checker\n",
    "def accuracy(predictions, true_y):\n",
    "    y = true_y.tolist() #convert to list format\n",
    "    correct = 0\n",
    "    for i in range(0, len(y)):\n",
    "        if predictions[i] == y[i]:\n",
    "            correct += 1\n",
    "    return correct/len(y)\n",
    "\n",
    "#precision, recall, and f1 score checker\n",
    "def precision_recall_F1_score(predictions, true_y):\n",
    "    TP, FP, FN = 0, 0, 0\n",
    "    y = true_y.tolist() #convert to list format\n",
    "    for i in range(0, len(y)):\n",
    "        true_val = y[i]\n",
    "        predicted_val = predictions[i]\n",
    "\n",
    "        if predicted_val == 1: #if positive predicted\n",
    "            if true_val == predicted_val: #positive correctly predicted\n",
    "                TP += 1\n",
    "            else: #positive incorrectly predicted\n",
    "                FP += 1\n",
    "        else: #negative predicted\n",
    "            if true_val != predicted_val: #negative incorrectly predicted\n",
    "                FN += 1 #negative incorrectly predicted\n",
    "                \n",
    "    precision = TP/(TP + FP+1e-10)\n",
    "    recall = TP/(TP + FN+1e-10)\n",
    "    f1_score = (2 * precision * recall)/(precision + recall+1e-10)\n",
    "    return precision, recall, f1_score"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "319e89d4",
   "metadata": {},
   "source": [
    "First let's try out a basic logistic regression model:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "68add0a9",
   "metadata": {},
   "outputs": [],
   "source": [
    "logistic_mod = linear_model.LogisticRegression().fit(X_train,y_train) #max_iter=2000\n",
    "pred_valid = logistic_mod.predict(X_valid)\n",
    "pred_test = logistic_mod.predict(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "ff342a74",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy:  0.9802816901408451\n",
      "Precision:  0.0\n",
      "Recall:  0.0\n",
      "f1_score:  0.0\n"
     ]
    }
   ],
   "source": [
    "#Validation set result:\n",
    "acc_valid = accuracy(pred_valid, y_valid)\n",
    "precision_valid, recall_valid, f1_score_valid = precision_recall_F1_score(pred_valid, y_valid) \n",
    "print(\"Accuracy: \", acc_valid)\n",
    "print(\"Precision: \", precision_valid)\n",
    "print(\"Recall: \", recall_valid)\n",
    "print(\"f1_score: \", f1_score_valid)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "9f684db4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy:  0.9759639459188784\n",
      "Precision:  0.0\n",
      "Recall:  0.0\n",
      "f1_score:  0.0\n"
     ]
    }
   ],
   "source": [
    "#Test set result:\n",
    "acc_test = accuracy(pred_test, y_test)\n",
    "precision_test, recall_test, f1_score_test = precision_recall_F1_score(pred_test, y_test) \n",
    "print(\"Accuracy: \", acc_test)\n",
    "print(\"Precision: \", precision_test)\n",
    "print(\"Recall: \", recall_test)\n",
    "print(\"f1_score: \", f1_score_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "df166ec2",
   "metadata": {},
   "source": [
    "Our accuracy seems really high but the rest of our performance measurers are at 0! What happened?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "21a34d72",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(0, 2)"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#printing the number of positives that were predicted from both sets:\n",
    "sum(pred_valid), sum(pred_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b1dab719",
   "metadata": {},
   "source": [
    "It seems like our model here had a strong preference for predicting non-bankruptcy on almost all of the predictions. That explains the very low precision and recall values respectively: the model we trained here is not great (frankly terrible) at predicting the bankruptcy values since it thinks better to just assume 0. This almost means we have 0 TPs, we didn't get any predictions on bankruptcy correct!. As such the the f1_score is pretty 0 due to the poor performance of the precision and recall scores.\n",
    "\n",
    "Overall accuracy is very high, but we're a lot more interested in predicting the bankruptcy cases then the non bankruptcy cases. In that sense, our base model is a failure. Now what if we applied some data augmentation methods?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8dfaabb5",
   "metadata": {},
   "source": [
    "# SMOTE (Synthetic Minority Over-sampling Technique)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "34cbc5e5",
   "metadata": {},
   "source": [
    "SMOTE works to help increase the imbalance between the number of classes. It works as follows:\n",
    " 1. Make note of the values that are part of the class with fewer values contained (in this case, the bankruptcy cases)\n",
    " \n",
    " 2. Run k-nearest neighbors on a bankruptcy value (think vectors) and selecting a random batch of these neighbors to generate synthetic values whose features lie between the original value and the neighbor value, repeatedly for each bankruptcy point\n",
    " \n",
    " 3. Add these new synthetic values to the dataset to bolster the bankruptcy cases\n",
    " \n",
    "This seems problematic at a glance but let's see how it works:\n",
    "(make note of the increased sample size below with SMOTE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "ad01ede3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Print X_train length 12778\n",
      "Print resampled X_train length 25006\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>year</th>\n",
       "      <th>A1</th>\n",
       "      <th>A2</th>\n",
       "      <th>A3</th>\n",
       "      <th>A4</th>\n",
       "      <th>A5</th>\n",
       "      <th>A6</th>\n",
       "      <th>A7</th>\n",
       "      <th>A8</th>\n",
       "      <th>A9</th>\n",
       "      <th>...</th>\n",
       "      <th>A55</th>\n",
       "      <th>A56</th>\n",
       "      <th>A57</th>\n",
       "      <th>A58</th>\n",
       "      <th>A59</th>\n",
       "      <th>A60</th>\n",
       "      <th>A61</th>\n",
       "      <th>A62</th>\n",
       "      <th>A63</th>\n",
       "      <th>A64</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>3</td>\n",
       "      <td>0.010548</td>\n",
       "      <td>0.28097</td>\n",
       "      <td>0.19354</td>\n",
       "      <td>1.8976</td>\n",
       "      <td>41.0800</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.010548</td>\n",
       "      <td>2.55900</td>\n",
       "      <td>1.0244</td>\n",
       "      <td>...</td>\n",
       "      <td>650.71</td>\n",
       "      <td>0.036687</td>\n",
       "      <td>0.01467</td>\n",
       "      <td>0.98972</td>\n",
       "      <td>0.079423</td>\n",
       "      <td>13.030</td>\n",
       "      <td>4.4091</td>\n",
       "      <td>76.826</td>\n",
       "      <td>4.7510</td>\n",
       "      <td>1.7338</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>0.056155</td>\n",
       "      <td>0.53111</td>\n",
       "      <td>0.31580</td>\n",
       "      <td>2.0809</td>\n",
       "      <td>32.4850</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.069669</td>\n",
       "      <td>0.88283</td>\n",
       "      <td>1.7005</td>\n",
       "      <td>...</td>\n",
       "      <td>11168.00</td>\n",
       "      <td>0.040438</td>\n",
       "      <td>0.11976</td>\n",
       "      <td>0.95962</td>\n",
       "      <td>0.084484</td>\n",
       "      <td>10.198</td>\n",
       "      <td>13.4680</td>\n",
       "      <td>62.712</td>\n",
       "      <td>5.8202</td>\n",
       "      <td>4.3379</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>1</td>\n",
       "      <td>0.190690</td>\n",
       "      <td>0.47717</td>\n",
       "      <td>0.15383</td>\n",
       "      <td>1.7175</td>\n",
       "      <td>3.0048</td>\n",
       "      <td>0.019519</td>\n",
       "      <td>0.238900</td>\n",
       "      <td>1.09570</td>\n",
       "      <td>1.6844</td>\n",
       "      <td>...</td>\n",
       "      <td>824.19</td>\n",
       "      <td>0.153740</td>\n",
       "      <td>0.36472</td>\n",
       "      <td>0.85820</td>\n",
       "      <td>0.470660</td>\n",
       "      <td>12.032</td>\n",
       "      <td>8.0748</td>\n",
       "      <td>46.458</td>\n",
       "      <td>7.8565</td>\n",
       "      <td>2.6661</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>2</td>\n",
       "      <td>0.049596</td>\n",
       "      <td>0.39135</td>\n",
       "      <td>0.14268</td>\n",
       "      <td>1.4014</td>\n",
       "      <td>-13.5750</td>\n",
       "      <td>0.149020</td>\n",
       "      <td>0.059049</td>\n",
       "      <td>1.24420</td>\n",
       "      <td>1.0357</td>\n",
       "      <td>...</td>\n",
       "      <td>6787.20</td>\n",
       "      <td>0.034483</td>\n",
       "      <td>0.10186</td>\n",
       "      <td>0.96552</td>\n",
       "      <td>0.073668</td>\n",
       "      <td>10.094</td>\n",
       "      <td>8.1199</td>\n",
       "      <td>61.666</td>\n",
       "      <td>5.9190</td>\n",
       "      <td>4.1928</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>4</td>\n",
       "      <td>0.068193</td>\n",
       "      <td>0.34917</td>\n",
       "      <td>0.23161</td>\n",
       "      <td>1.7112</td>\n",
       "      <td>-28.7140</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.068193</td>\n",
       "      <td>1.86390</td>\n",
       "      <td>1.1223</td>\n",
       "      <td>...</td>\n",
       "      <td>1375.40</td>\n",
       "      <td>-0.110310</td>\n",
       "      <td>0.10478</td>\n",
       "      <td>1.17090</td>\n",
       "      <td>0.011255</td>\n",
       "      <td>4.712</td>\n",
       "      <td>3.5793</td>\n",
       "      <td>105.920</td>\n",
       "      <td>3.4460</td>\n",
       "      <td>2.5351</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows × 65 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "   year        A1       A2       A3      A4       A5        A6        A7  \\\n",
       "0     3  0.010548  0.28097  0.19354  1.8976  41.0800  0.000000  0.010548   \n",
       "1     1  0.056155  0.53111  0.31580  2.0809  32.4850  0.000000  0.069669   \n",
       "2     1  0.190690  0.47717  0.15383  1.7175   3.0048  0.019519  0.238900   \n",
       "3     2  0.049596  0.39135  0.14268  1.4014 -13.5750  0.149020  0.059049   \n",
       "4     4  0.068193  0.34917  0.23161  1.7112 -28.7140  0.000000  0.068193   \n",
       "\n",
       "        A8      A9  ...       A55       A56      A57      A58       A59  \\\n",
       "0  2.55900  1.0244  ...    650.71  0.036687  0.01467  0.98972  0.079423   \n",
       "1  0.88283  1.7005  ...  11168.00  0.040438  0.11976  0.95962  0.084484   \n",
       "2  1.09570  1.6844  ...    824.19  0.153740  0.36472  0.85820  0.470660   \n",
       "3  1.24420  1.0357  ...   6787.20  0.034483  0.10186  0.96552  0.073668   \n",
       "4  1.86390  1.1223  ...   1375.40 -0.110310  0.10478  1.17090  0.011255   \n",
       "\n",
       "      A60      A61      A62     A63     A64  \n",
       "0  13.030   4.4091   76.826  4.7510  1.7338  \n",
       "1  10.198  13.4680   62.712  5.8202  4.3379  \n",
       "2  12.032   8.0748   46.458  7.8565  2.6661  \n",
       "3  10.094   8.1199   61.666  5.9190  4.1928  \n",
       "4   4.712   3.5793  105.920  3.4460  2.5351  \n",
       "\n",
       "[5 rows x 65 columns]"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from imblearn.over_sampling import SMOTE\n",
    "\n",
    "smote = SMOTE(sampling_strategy='auto')\n",
    "X_resampled_train, y_resampled_train = smote.fit_resample(X_train, y_train)\n",
    "\n",
    "print(\"Print X_train length\", len(X_train))\n",
    "print(\"Print resampled X_train length\", len(X_resampled_train))\n",
    "\n",
    "X_resampled_train.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f63047b1",
   "metadata": {},
   "source": [
    "Let's try running a model here now:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "91ccb474",
   "metadata": {},
   "outputs": [],
   "source": [
    "logistic_mod_smote = linear_model.LogisticRegression().fit(X_resampled_train,y_resampled_train)\n",
    "pred_valid = logistic_mod_smote.predict(X_valid)\n",
    "pred_test = logistic_mod_smote.predict(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "7ab0ac7d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy:  0.7276995305164319\n",
      "Precision:  0.037800687285219035\n",
      "Recall:  0.5238095238086924\n",
      "f1_score:  0.07051282050024997\n"
     ]
    }
   ],
   "source": [
    "#Validation set result:\n",
    "acc_valid = accuracy(pred_valid, y_valid)\n",
    "precision_valid, recall_valid, f1_score_valid = precision_recall_F1_score(pred_valid, y_valid) \n",
    "print(\"Accuracy: \", acc_valid)\n",
    "print(\"Precision: \", precision_valid)\n",
    "print(\"Recall: \", recall_valid)\n",
    "print(\"f1_score: \", f1_score_valid)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "9930797d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy:  0.7263395092638958\n",
      "Precision:  0.038781163434899464\n",
      "Recall:  0.4468085106378225\n",
      "f1_score:  0.07136788443728731\n"
     ]
    }
   ],
   "source": [
    "#Test set result:\n",
    "acc_test = accuracy(pred_test, y_test)\n",
    "precision_test, recall_test, f1_score_test = precision_recall_F1_score(pred_test, y_test) \n",
    "print(\"Accuracy: \", acc_test)\n",
    "print(\"Precision: \", precision_test)\n",
    "print(\"Recall: \", recall_test)\n",
    "print(\"f1_score: \", f1_score_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "89188694",
   "metadata": {},
   "source": [
    "Our accuracy dropped in the validation and test but we don't have 0 values for any of the other measurers anymore! Recall went up significantly, so we're clearly able to better indicate most of the bankruptcy cases in either set but our precision is still pretty small. We can figure out what happened here by checking the value counts of how many bankruptcy cases we really have in both sets:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "080bbc2c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(873, 63)"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sum(pred_valid), sum(y_valid)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "9e69b13b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(1083, 94)"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sum(pred_test), sum(y_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d29298a4",
   "metadata": {},
   "source": [
    "Now we can see we did end up inflating the number of bankruptcy predictions we made but by too much! We have more than 10 times the number of actual bankruptcy cases here in the predictions. That explains why our precision is so low still, we're having issues truly predicting one that would exist in an unknown set in comparison to just finding the one in our given test/validation set here. This is to be expected since SMOTE also introduced a lot of noise here which is why we saw such an increase in bankruptcy predictions for our model."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "924c8f92-1067-45cf-a2de-c95d87fbe2d0",
   "metadata": {},
   "source": [
    "# Noise Injection to features"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "27d02166-0810-419a-b7cc-2bac00295385",
   "metadata": {},
   "source": [
    "The technique sounds exactly as it describes; we're introducing a certain amount of noise to our features that we train in. This sounds like a negative practice at first but in reality, the model we train learns the noise as a sort of invariability to the data which helps out the imbalance class issues we have here"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "34da200d-58d7-4bd2-a89b-6fce38e238f2",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "# Injecting Gaussian noise (mean=0, standard deviation=1)\n",
    "noise_factor = 1000 #0.01, 0.1, 1, 10, 100, 1000\n",
    "X_train_noisy = X_train + noise_factor * np.random.normal(loc=0.0, scale=1.0, size=X_train.shape)\n",
    "\n",
    "# Clip the values to ensure no negative values where it doesn't make sense\n",
    "X_train_noisy = np.clip(X_train_noisy, 0, None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "0b2ce731-74cc-4c11-a289-09e591fd3d1a",
   "metadata": {},
   "outputs": [],
   "source": [
    "logistic_mod_noisy = linear_model.LogisticRegression().fit(X_train_noisy,y_train)\n",
    "pred_valid = logistic_mod_noisy.predict(X_valid)\n",
    "pred_test = logistic_mod_noisy.predict(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "9df7106d-8bce-4421-a570-1e32e6014c0e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy:  0.8031298904538341\n",
      "Precision:  0.03453947368420484\n",
      "Recall:  0.33333333333280424\n",
      "f1_score:  0.06259314454332413\n"
     ]
    }
   ],
   "source": [
    "#Validation set result:\n",
    "acc_valid = accuracy(pred_valid, y_valid)\n",
    "precision_valid, recall_valid, f1_score_valid = precision_recall_F1_score(pred_valid, y_valid) \n",
    "print(\"Accuracy: \", acc_valid)\n",
    "print(\"Precision: \", precision_valid)\n",
    "print(\"Recall: \", recall_valid)\n",
    "print(\"f1_score: \", f1_score_valid)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "14faa9ea-e537-4b1e-bed7-6dfab8cfb560",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy:  0.800701051577366\n",
      "Precision:  0.029490616621979958\n",
      "Recall:  0.23404255319124037\n",
      "f1_score:  0.052380952361063486\n"
     ]
    }
   ],
   "source": [
    "#Test set result:\n",
    "acc_test = accuracy(pred_test, y_test)\n",
    "precision_test, recall_test, f1_score_test = precision_recall_F1_score(pred_test, y_test) \n",
    "print(\"Accuracy: \", acc_test)\n",
    "print(\"Precision: \", precision_test)\n",
    "print(\"Recall: \", recall_test)\n",
    "print(\"f1_score: \", f1_score_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5526567e-3ada-4606-accc-c7aa4862dafd",
   "metadata": {},
   "source": [
    "On the plus side, noise factor doesn't inflate the sample set as as hard SMOTE but the result improvements are much scale in comparison. We didn't see that much improvement in recall as we did previously, though we did improve precision about the same amount and accuracy wasn't reduced as severely. It's best to not rely solely on Noise Injection to fix this imbalance"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "90f00815-6eff-4f13-b3e6-573f642b30c6",
   "metadata": {},
   "source": [
    "# Feature Augmentation"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5bb8c0ca-a879-4181-863c-6f549a1360c3",
   "metadata": {},
   "source": [
    "Sometimes our data feature may have inherent relationships between one another that may not necessarily be clear and/or easy to fit to the data classes. For example there may not exist direct features that can 1 to 1 predict whether a bank data value will become bankrupt in the future or not but what if there was some complex mathematical combination of features that we could discover that could do so? This can prove useful to predicting data with little correlations to features but can be time intensive due to the number of potential combinations one must test along with the expansion of new feature columns to each data value."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "1427dab4-9003-4cf3-ad02-e78e037263cb",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import PolynomialFeatures\n",
    "\n",
    "# Generate interaction terms and polynomial features (degree=2 for pairwise interactions)\n",
    "poly = PolynomialFeatures(degree=2, include_bias=False)\n",
    "X_train_poly = poly.fit_transform(X_train)\n",
    "X_valid_poly = poly.fit_transform(X_valid)\n",
    "X_test_poly = poly.fit_transform(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "5d36511a-b9df-4edd-a58a-686e2b8b509d",
   "metadata": {},
   "outputs": [],
   "source": [
    "logistic_mod_feature = linear_model.LogisticRegression().fit(X_train_poly,y_train)\n",
    "pred_valid = logistic_mod_feature.predict(X_valid_poly)\n",
    "pred_test = logistic_mod_feature.predict(X_test_poly)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "a847d1c4-632b-441e-b370-e0f6ba3aee3f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy:  0.9236306729264476\n",
      "Precision:  0.0867579908675403\n",
      "Recall:  0.30158730158682284\n",
      "f1_score:  0.13475177301485086\n"
     ]
    }
   ],
   "source": [
    "#Validation set result:\n",
    "acc_valid = accuracy(pred_valid, y_valid)\n",
    "precision_valid, recall_valid, f1_score_valid = precision_recall_F1_score(pred_valid, y_valid) \n",
    "print(\"Accuracy: \", acc_valid)\n",
    "print(\"Precision: \", precision_valid)\n",
    "print(\"Recall: \", recall_valid)\n",
    "print(\"f1_score: \", f1_score_valid)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "e1382f08-87df-4f03-97f3-b948b5ac54d8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy:  0.9263895843765648\n",
      "Precision:  0.08333333333329862\n",
      "Recall:  0.21276595744658217\n",
      "f1_score:  0.1197604790013984\n"
     ]
    }
   ],
   "source": [
    "#Test set result:\n",
    "acc_test = accuracy(pred_test, y_test)\n",
    "precision_test, recall_test, f1_score_test = precision_recall_F1_score(pred_test, y_test) \n",
    "print(\"Accuracy: \", acc_test)\n",
    "print(\"Precision: \", precision_test)\n",
    "print(\"Recall: \", recall_test)\n",
    "print(\"f1_score: \", f1_score_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "63cf2e29-fe19-42c8-8df3-4334d526c757",
   "metadata": {},
   "source": [
    "Based on the results here, it seems we could find any combination of polynomial functions to the 2nd degree that could help improve our measurers here in comparison to our other two methods. Due to the longer run time here, we likely shouldn't focus too much on this method if our improvements were barely made for longer time taken. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fc20cd6b-3697-4fb7-ac6b-9413808fce90",
   "metadata": {},
   "source": [
    "# Targeted Undersampling (for Class Balance)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b655f77b-abdf-4caf-baa6-9b0322b6b8c0",
   "metadata": {},
   "source": [
    "Similar to how SMOTE creates more of the minority classes values, we can also a data augmentation technique to reduce the amount of majority class values to help with imbalance. The concepts and ideas are generally the same with similar pros and cons. Here we use the RandomUnderSampler which chooses a random subset of points from the majority class to be removed. Note this is the main difference to something like SMOTE which increases the number of values wheareas here we decrease them."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "755f3e84-7bbe-4398-920e-37d0eda6bca2",
   "metadata": {},
   "outputs": [],
   "source": [
    "rus = RandomUnderSampler(random_state=42)\n",
    "X_train_resampled, y_train_resampled = rus.fit_resample(X_train, y_train)\n",
    "logistic_mod_rus = linear_model.LogisticRegression().fit(X_train_resampled,y_train_resampled)\n",
    "pred_valid = logistic_mod_rus.predict(X_valid)\n",
    "pred_test = logistic_mod_rus.predict(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "792b637e-1da9-424f-9812-7f424500b053",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "12778\n",
      "550\n"
     ]
    }
   ],
   "source": [
    "print(len(X_train))\n",
    "print(len(X_train_resampled))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "c1f1b727-43ce-4d5c-87fb-a679dde9d171",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy:  0.5859154929577465\n",
      "Precision:  0.03194650817236018\n",
      "Recall:  0.6825396825385991\n",
      "f1_score:  0.06103619587505406\n"
     ]
    }
   ],
   "source": [
    "#Validation set result:\n",
    "acc_valid = accuracy(pred_valid, y_valid)\n",
    "precision_valid, recall_valid, f1_score_valid = precision_recall_F1_score(pred_valid, y_valid) \n",
    "print(\"Accuracy: \", acc_valid)\n",
    "print(\"Precision: \", precision_valid)\n",
    "print(\"Recall: \", recall_valid)\n",
    "print(\"f1_score: \", f1_score_valid)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "e1d274b1-2c82-45ca-9617-8416bf4b176a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy:  0.5843765648472709\n",
      "Precision:  0.03723404255318929\n",
      "Recall:  0.6702127659567338\n",
      "f1_score:  0.07054871219606683\n"
     ]
    }
   ],
   "source": [
    "#Test set result:\n",
    "acc_test = accuracy(pred_test, y_test)\n",
    "precision_test, recall_test, f1_score_test = precision_recall_F1_score(pred_test, y_test) \n",
    "print(\"Accuracy: \", acc_test)\n",
    "print(\"Precision: \", precision_test)\n",
    "print(\"Recall: \", recall_test)\n",
    "print(\"f1_score: \", f1_score_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e801541a-2692-412f-8a07-ba0067e221d2",
   "metadata": {},
   "source": [
    "Interestingly, it seems the results here are quite similar to what was achieved with SMOTE. We have a very much improved recall but we're making a lot more incorrect predictions for bankruptcy still, and we have a reduction in accuracy as well."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e6e80d7a",
   "metadata": {},
   "source": [
    "# Combined Techniques\n",
    "\n",
    "Now let's apply all of these methods all on one singular model:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "b01472ff",
   "metadata": {},
   "outputs": [],
   "source": [
    "def add_noise(X, noise_level=0.1):\n",
    "    noise = np.random.normal(loc=0, scale=noise_level, size=X.shape)\n",
    "    return X + noise"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "12bc8dbd",
   "metadata": {},
   "outputs": [],
   "source": [
    "#SMOTe\n",
    "smote = SMOTE(sampling_strategy='auto')\n",
    "X_smote_train, y_smote_train = smote.fit_resample(X_train, y_train)\n",
    "\n",
    "#RUS\n",
    "rus = RandomUnderSampler(random_state=42)\n",
    "X_train_resampled, y_train_resampled = rus.fit_resample(X_smote_train, y_smote_train)\n",
    "\n",
    "#noise injection\n",
    "X_resampled_noisy = add_noise(X_train_resampled, noise_level=0.05)\n",
    "\n",
    "#polynomial features\n",
    "poly = PolynomialFeatures(degree=2, interaction_only=True, include_bias=False)\n",
    "X_train_poly = poly.fit_transform(X_resampled_noisy)\n",
    "X_valid_poly = poly.fit_transform(X_valid)\n",
    "X_test_poly = poly.fit_transform(X_test)\n",
    "\n",
    "logistic_mod_feature = linear_model.LogisticRegression().fit(X_train_poly,y_train_resampled)\n",
    "pred_valid = logistic_mod_feature.predict(X_valid_poly)\n",
    "pred_test = logistic_mod_feature.predict(X_test_poly)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "17798d3c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy:  0.6225352112676056\n",
      "Precision:  0.033469387755099306\n",
      "Recall:  0.6507936507926178\n",
      "f1_score:  0.06366459626397793\n"
     ]
    }
   ],
   "source": [
    "#Validation set result:\n",
    "acc_valid = accuracy(pred_valid, y_valid)\n",
    "precision_valid, recall_valid, f1_score_valid = precision_recall_F1_score(pred_valid, y_valid) \n",
    "print(\"Accuracy: \", acc_valid)\n",
    "print(\"Precision: \", precision_valid)\n",
    "print(\"Recall: \", recall_valid)\n",
    "print(\"f1_score: \", f1_score_valid)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "0ffe5905",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy:  0.6319479218828242\n",
      "Precision:  0.04316069057104627\n",
      "Recall:  0.691489361701392\n",
      "f1_score:  0.08124999998893015\n"
     ]
    }
   ],
   "source": [
    "#Test set result:\n",
    "acc_test = accuracy(pred_test, y_test)\n",
    "precision_test, recall_test, f1_score_test = precision_recall_F1_score(pred_test, y_test) \n",
    "print(\"Accuracy: \", acc_test)\n",
    "print(\"Precision: \", precision_test)\n",
    "print(\"Recall: \", recall_test)\n",
    "print(\"f1_score: \", f1_score_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "94a0fde2",
   "metadata": {},
   "source": [
    "These results ended up being the most balanced out of what we got, seeing as we have the highest combination of accuracy and recall, along with moderately high precision and f1_score in comparison to our other models. Of course, we still hit quite low values for precision and f1_score so we can always improve to something more ideal by changing methods or tuning the hyperparameters. Nevertheless, data augmentation in this case has proven to help remedy and train models such that they can understand the data they're working with better by factoring noise and classifications with weight. Now that we highlighted these benefits, let's try something more complex. "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
