{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "5cc5cb51-4f35-4236-8250-860f5b804bc5",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import copy\n",
    "import pickle\n",
    "import sympy\n",
    "import functools\n",
    "import itertools\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "from error_injection import MissingValueError, SamplingError, Injector\n",
    "from sklearn.decomposition import PCA\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.experimental import enable_iterative_imputer\n",
    "from sklearn.impute import SimpleImputer, KNNImputer, IterativeImputer\n",
    "from sklearn.metrics import mutual_info_score, auc, roc_curve, roc_auc_score, f1_score\n",
    "from scipy.optimize import minimize as scipy_min\n",
    "from scipy.spatial import ConvexHull\n",
    "from scipy.optimize import minimize, Bounds, linprog\n",
    "from sympy import Symbol as sb\n",
    "from sympy import lambdify\n",
    "from tqdm.notebook import trange,tqdm\n",
    "from IPython.display import display,clear_output\n",
    "from random import choice\n",
    "from sklearn.linear_model import LinearRegression\n",
    "from sklearn.utils import resample\n",
    "from sklearn.ensemble import RandomForestRegressor\n",
    "from sklearn.linear_model import Ridge\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.tree import _tree\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "from sklearn.metrics import make_scorer, recall_score\n",
    "\n",
    "class style():\n",
    "    RED = '\\033[31m'\n",
    "    GREEN = '\\033[32m'\n",
    "    BLUE = '\\033[34m'\n",
    "    RESET = '\\033[0m'\n",
    "\n",
    "np.random.seed(1)\n",
    "\n",
    "# ignore all the warnings\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "954548e9-fa14-4b68-9bca-6f17dd409434",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1070"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def load_ins_cleaned():\n",
    "    # fetch dataset\n",
    "    auto_mpg = pd.read_csv('datasets/insurance.csv').drop('sex', axis=1).drop('smoker', axis=1).drop('region', axis=1).replace('?', np.nan)\n",
    "    features = ['age', 'bmi', 'children']\n",
    "    X = auto_mpg[features].astype(float)\n",
    "    y = auto_mpg['charges']\n",
    "    \n",
    "    # assumed gt imputation\n",
    "    imputer = KNNImputer(n_neighbors=10)\n",
    "    X = pd.DataFrame(imputer.fit_transform(X), columns=X.columns)\n",
    "\n",
    "    X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=1)\n",
    "    X_train = copy.deepcopy(X_train).reset_index(drop=True)\n",
    "    X_test = copy.deepcopy(X_test).reset_index(drop=True)\n",
    "    y_train = y_train.reset_index(drop=True)\n",
    "    y_test = y_test.reset_index(drop=True)\n",
    "\n",
    "    return X_train, X_test, y_train, y_test\n",
    "\n",
    "X_train_ins, X_test_ins, y_train_ins, y_test_ins = load_ins_cleaned()\n",
    "len(X_train_ins)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "65730f27-8d6c-4b3d-b78d-80d8b5b03fa0",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "318"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# first impute the data and make it hypothetically clean\n",
    "def load_mpg_cleaned():\n",
    "    # fetch dataset\n",
    "    auto_mpg = pd.read_csv('datasets/auto-mpg.csv').drop('car name', axis=1).replace('?', np.nan)\n",
    "    \n",
    "    features = ['cylinders', 'displacement', 'horsepower', 'weight',\n",
    "                'acceleration', 'model year', 'origin']\n",
    "    X = auto_mpg[features].astype(float)\n",
    "    y = auto_mpg['mpg']\n",
    "    \n",
    "    # assumed gt imputation\n",
    "    imputer = KNNImputer(n_neighbors=10)\n",
    "    X = pd.DataFrame(imputer.fit_transform(X), columns=X.columns)\n",
    "\n",
    "    X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=1)\n",
    "    X_train = copy.deepcopy(X_train).reset_index(drop=True)\n",
    "    X_test = copy.deepcopy(X_test).reset_index(drop=True)\n",
    "    y_train = y_train.reset_index(drop=True)\n",
    "    y_test = y_test.reset_index(drop=True)\n",
    "\n",
    "    return X_train, X_test, y_train, y_test\n",
    "X_train_mpg, X_test_mpg, y_train_mpg, y_test_mpg = load_mpg_cleaned()\n",
    "len(X_train_mpg)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "645ee5ee-ba7a-431b-a5a5-469cb8082516",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 506 entries, 0 to 505\n",
      "Data columns (total 14 columns):\n",
      " #   Column   Non-Null Count  Dtype  \n",
      "---  ------   --------------  -----  \n",
      " 0   CRIM     506 non-null    float64\n",
      " 1   ZN       506 non-null    float64\n",
      " 2   INDUS    506 non-null    float64\n",
      " 3   CHAS     506 non-null    int64  \n",
      " 4   NOX      506 non-null    float64\n",
      " 5   RM       506 non-null    float64\n",
      " 6   AGE      506 non-null    float64\n",
      " 7   DIS      506 non-null    float64\n",
      " 8   RAD      506 non-null    int64  \n",
      " 9   TAX      506 non-null    float64\n",
      " 10  PTRATIO  506 non-null    float64\n",
      " 11  B        506 non-null    float64\n",
      " 12  LSTAT    506 non-null    float64\n",
      " 13  MEDV     506 non-null    float64\n",
      "dtypes: float64(12), int64(2)\n",
      "memory usage: 55.5 KB\n"
     ]
    }
   ],
   "source": [
    "column_names = ['CRIM', 'ZN', 'INDUS', 'CHAS', 'NOX', 'RM', 'AGE', 'DIS', 'RAD', 'TAX', 'PTRATIO', 'B', 'LSTAT', 'MEDV']\n",
    "boston_df = pd.read_csv('datasets/housing.csv', header=None, delimiter=r\"\\s+\", names=column_names)\n",
    "boston_df.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "db02830a-9a35-4da8-9b7c-fca24979e909",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Useful functions\n",
    "symbol_id = -1\n",
    "def create_symbol(suffix=''):\n",
    "    global symbol_id\n",
    "    symbol_id += 1\n",
    "    name = f'e{symbol_id}_{suffix}' if suffix else f'e{symbol_id}'\n",
    "    return sympy.Symbol(name=name)\n",
    "\n",
    "\n",
    "#scaler_symbols = set([sb(f'k{i}') for i in range(X_train.shape[1]+1)])\n",
    "#linearization_dict = dict()\n",
    "#reverse_linearization_dict = dict()\n",
    "\n",
    "def inject_sensitive_ranges(X, y, uncertain_attr, uncertain_num, boundary_indices, uncertain_radius_pct=None, \n",
    "                  uncertain_radius=None, seed=42):\n",
    "    global symbol_id\n",
    "    symbol_id = -1\n",
    "    \n",
    "    X_extended = np.append(np.ones((len(X), 1)), X, axis=1)\n",
    "    ss = StandardScaler()\n",
    "    X_extended[:, 1:] = ss.fit_transform(X_extended[:, 1:])\n",
    "    X_extended_symb = sympy.Matrix(X_extended)\n",
    "    \n",
    "    if not(uncertain_attr=='y'):\n",
    "        uncertain_attr_idx = X.columns.to_list().index(uncertain_attr) + 1\n",
    "        if not(uncertain_radius):\n",
    "            uncertain_radius = uncertain_radius_pct*(np.max(X_extended[:, uncertain_attr_idx])-\\\n",
    "                                                     np.min(X_extended[:, uncertain_attr_idx]))\n",
    "    else:\n",
    "        if not(uncertain_radius):\n",
    "            uncertain_radius = uncertain_radius_pct*(y_train.max()-y_train.min())[0]\n",
    "    \n",
    "    np.random.seed(seed)\n",
    "    uncertain_indices = boundary_indices[:uncertain_num]\n",
    "    y_symb = sympy.Matrix(y)\n",
    "    symbols_in_data = set()\n",
    "    #print(uncertain_indices)\n",
    "    for uncertain_idx in uncertain_indices:\n",
    "        new_symb = create_symbol()\n",
    "        symbols_in_data.add(new_symb)\n",
    "        if uncertain_attr=='y':\n",
    "            y_symb[uncertain_idx] = y_symb[uncertain_idx] + uncertain_radius*new_symb\n",
    "        else:\n",
    "            X_extended_symb[uncertain_idx, uncertain_attr_idx] = X_extended_symb[uncertain_idx, uncertain_attr_idx] + uncertain_radius*new_symb\n",
    "    return X_extended_symb, y_symb, symbols_in_data, ss\n",
    "\n",
    "# if interval=True, use interval arithmetic, otherwise use zonotopes\n",
    "def compute_robustness_ratio_sensitive_label_error(X_train, y_train, X_test, y_test, robustness_radius,\n",
    "                                         uncertain_num, boundary_indices, uncertain_radius=None, \n",
    "                                         lr=0.1, seed=42, interval=True):\n",
    "    X, y, symbols_in_data, ss = inject_sensitive_ranges(X=X_train, y=y_train, uncertain_attr='y', \n",
    "                                              uncertain_num=uncertain_num, boundary_indices=boundary_indices, \n",
    "                                              uncertain_radius=uncertain_radius, \n",
    "                                              uncertain_radius_pct=None, seed=seed)\n",
    "    \n",
    "    assert len(X.free_symbols)==0\n",
    "    # closed-form\n",
    "    param = (X.T*X).inv()*X.T*y\n",
    "    \n",
    "    if interval:\n",
    "        # make param intervals\n",
    "        for d in range(len(param)):\n",
    "            expr = param[d]\n",
    "            if not(expr.free_symbols):\n",
    "                continue\n",
    "            else:\n",
    "                constant_part = 0\n",
    "                interval_radius = 0\n",
    "                for arg in expr.args:\n",
    "                    if arg.free_symbols:\n",
    "                        interval_radius += abs(arg.args[0])\n",
    "                    else:\n",
    "                        assert constant_part == 0\n",
    "                        constant_part = arg\n",
    "                param[d] = constant_part + create_symbol()*interval_radius\n",
    "    \n",
    "    test_preds = sympy.Matrix(np.append(np.ones((len(X_test), 1)), ss.transform(X_test), axis=1))*param\n",
    "    robustness_ls = []\n",
    "    for pred in test_preds:\n",
    "        pred_range_radius = 0\n",
    "        for arg in pred.args:\n",
    "            if arg.free_symbols:\n",
    "                pred_range_radius += abs(arg.args[0])\n",
    "        if pred_range_radius <= robustness_radius:\n",
    "            robustness_ls.append(1)\n",
    "        else:\n",
    "            robustness_ls.append(0)\n",
    "    \n",
    "#     print(param)\n",
    "    return np.mean(robustness_ls)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "4629de4b-2b60-46ca-bdab-f79dd649632f",
   "metadata": {},
   "outputs": [],
   "source": [
    "#accuracy r2 = True; rmse, mse = False for maximize\n",
    "def leave_one_out(X_train, y_train, X_test, y_test, model, metric, maximize=True): \n",
    "    predictions = model.fit(X_train, y_train).predict(X_test)\n",
    "    initial_metric = metric(y_test.to_numpy(), predictions)\n",
    "    influence_results = []\n",
    "   \n",
    "    for i in range(len(X_train)):\n",
    "        X_train_new = np.delete(X_train, i, axis=0)\n",
    "        y_train_new = np.delete(y_train, i, axis=0)\n",
    "       \n",
    "        model_clone = model.__class__(**model.get_params())\n",
    "        new_preds = model_clone.fit(X_train_new, y_train_new).predict(X_test)\n",
    "        new_metric = metric(y_test.to_numpy(), new_preds)\n",
    "       \n",
    "        metric_diff = (initial_metric - new_metric) if maximize else (new_metric - initial_metric)\n",
    "        \n",
    "        influence_results.append((i, metric_diff))\n",
    "       \n",
    "       \n",
    "   \n",
    "    influence_results = sorted(influence_results,key=lambda x: x[1], reverse=True)\n",
    "    #print(influence_results)\n",
    "    return [i[0] for i in influence_results]\n",
    "\n",
    "def mae(y_true, y_pred):\n",
    "    return sum(abs(y_true - y_pred))/len(y_true)\n",
    "\n",
    "def mse(y_true, y_pred):\n",
    "    return sum((y_true - y_pred)**2)/len(y_true)\n",
    "\n",
    "def r_squared(y_true, y_pred):\n",
    "    y_bar = np.mean(y_true)\n",
    "    return 1 -(sum((y_true - y_pred)**2)/sum((y_true-y_bar)**2))\n",
    "\n",
    "def rmse(y_true, y_pred):\n",
    "    return np.sqrt(sum((y_true - y_pred)**2)/len(y_true))\n",
    "\n",
    "lr = LinearRegression()\n",
    "\n",
    "X_train_ins, X_test_ins, y_train_ins, y_test_ins = X_train_ins.reset_index(drop=True) , X_test_ins.reset_index(drop=True) , y_train_ins.reset_index(drop=True) , y_test_ins.reset_index(drop=True)\n",
    "boundary_indices_ins = leave_one_out(X_train_ins, y_train_ins, X_test_ins, y_test_ins, lr, mse, maximize=False)\n",
    "\n",
    "X_train_mpg, X_test_mpg, y_train_mpg, y_test_mpg = X_train_mpg.reset_index(drop=True) , X_test_mpg.reset_index(drop=True) , y_train_mpg.reset_index(drop=True) , y_test_mpg.reset_index(drop=True)\n",
    "boundary_indices_mpg = leave_one_out(X_train_mpg, y_train_mpg, X_test_mpg, y_test_mpg, lr, mse, maximize=False)\n",
    "\n",
    "boundary_indices_lst = [boundary_indices_ins, boundary_indices_mpg]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "a9fd239b-3f48-4d84-a4e8-09079ebf8b84",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training data shape: (404, 13)\n",
      "Testing data shape: (102, 13)\n"
     ]
    }
   ],
   "source": [
    "X = boston_df.drop(columns=['MEDV'])\n",
    "y = boston_df['MEDV']\n",
    "X_train_bos, X_test_bos, y_train_bos, y_test_bos = train_test_split(X, y, test_size=0.2, random_state=1)\n",
    "\n",
    "print(\"Training data shape:\", X_train_bos.shape)\n",
    "print(\"Testing data shape:\", X_test_bos.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "9a77d36d-bc38-40ac-a40b-3ecb28c1e8b5",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'LSTAT'"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def select_best_feature(X, y, method=\"correlation\"):\n",
    "    if method == \"correlation\":\n",
    "        correlations = X.corrwith(y)\n",
    "        best_feature = correlations.abs().idxmax()\n",
    "    return best_feature\n",
    "\n",
    "best_feature = select_best_feature(X_train_bos, y_train_bos)\n",
    "best_feature"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "036edda9-a729-49e6-8b05-d2e5c10ad092",
   "metadata": {},
   "outputs": [],
   "source": [
    "lr = LinearRegression()\n",
    "X_train_bos, X_test_bos, y_train_bos, y_test_bos = X_train_bos.reset_index(drop=True) , X_test_bos.reset_index(drop=True) , y_train_bos.reset_index(drop=True) , y_test_bos.reset_index(drop=True)\n",
    "boundary_indices_bos = leave_one_out(X_train_bos, y_train_bos, X_test_bos, y_test_bos, lr, mae, maximize=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "31546bc8-ae43-455c-aaf0-b6fb3cdafd82",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[(0, 12.130000114440918)]"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Decision Tree research: 1% of the data\n",
    "array_indexes = np.zeros(len(X_train_bos))\n",
    "perc = 0.1 * len(X_train_bos)\n",
    "for i in range(0, len(X_train_bos)):\n",
    "    if i <= perc:\n",
    "        index = boundary_indices_bos[i]\n",
    "        array_indexes[index] = 1\n",
    "\n",
    "clf = DecisionTreeClassifier(max_depth=None)\n",
    "clf.fit(X_train_bos, array_indexes)\n",
    "\n",
    "feature_max_values = X_train_bos.max()\n",
    "\n",
    "def get_positive_paths(tree, feature_names, node=0, depth=0, conditions=None, results=None, min_positive_ratio=0.5):\n",
    "    if conditions is None:\n",
    "        conditions = {}\n",
    "    if results is None:\n",
    "        results = []\n",
    "\n",
    "    left_child = tree.children_left[node]\n",
    "    right_child = tree.children_right[node]\n",
    "    threshold = tree.threshold[node]\n",
    "    feature = tree.feature[node]\n",
    "\n",
    "    # Count samples in this node\n",
    "    sample_count = int(tree.n_node_samples[node])\n",
    "    positive_count = int(tree.value[node][0, 1]) if tree.n_outputs == 1 else int(tree.value[node][0][1])\n",
    "    negative_count = int(tree.value[node][0, 0]) if tree.n_outputs == 1 else int(tree.value[node][0][0])\n",
    "\n",
    "    # Calculate the positive ratio for this node\n",
    "    positive_ratio = positive_count / sample_count if sample_count > 0 else 0\n",
    "\n",
    "    # If it's a leaf or qualifies as a 'positive node' by ratio, store the path\n",
    "    if (left_child == _tree.TREE_LEAF and right_child == _tree.TREE_LEAF) or positive_ratio >= min_positive_ratio:\n",
    "        path_conditions = {}\n",
    "        for feat, bounds in conditions.items():\n",
    "            lower_bound = bounds.get('lower', 0)\n",
    "            upper_bound = bounds.get('upper', feature_max_values.get(feat, '∞'))  # Use the max value for the feature\n",
    "            path_conditions[feat] = (lower_bound, upper_bound)\n",
    "        \n",
    "        # Only store if there are significant positives\n",
    "        if positive_count > 0:  # Ensure that there's at least one positive sample\n",
    "            results.append((positive_count, sample_count, path_conditions, positive_ratio, depth))\n",
    "\n",
    "    # Update bounds for the current feature in conditions and recurse\n",
    "    feature_name = feature_names[feature] if feature != _tree.TREE_UNDEFINED else None\n",
    "    if left_child != _tree.TREE_LEAF and feature_name:\n",
    "        # Left child represents the <= threshold split\n",
    "        new_conditions = {k: v.copy() for k, v in conditions.items()}\n",
    "        new_conditions.setdefault(feature_name, {}).update({'upper': threshold})\n",
    "        get_positive_paths(tree, feature_names, left_child, depth + 1, new_conditions, results, min_positive_ratio)\n",
    "\n",
    "    if right_child != _tree.TREE_LEAF and feature_name:\n",
    "        # Right child represents the > threshold split\n",
    "        new_conditions = {k: v.copy() for k, v in conditions.items()}\n",
    "        new_conditions.setdefault(feature_name, {}).update({'lower': threshold})\n",
    "        get_positive_paths(tree, feature_names, right_child, depth + 1, new_conditions, results, min_positive_ratio)\n",
    "\n",
    "    # Print and store paths after completing all nodes, if we are at the root node\n",
    "    if node == 0:\n",
    "        # Sort results first by depth (root to leaf), then by positive ratio, and then by positive count\n",
    "        top_results = sorted(results, key=lambda x: (x[4], x[3], x[0]), reverse=False)[:3]  # Prioritize by depth first\n",
    "        \n",
    "        # Store top thresholds\n",
    "        top_thresholds = []\n",
    "        for idx, (pos_count, total_count, conditions, pos_ratio, dep) in enumerate(top_results, start=1):\n",
    "            top_thresholds.append(conditions)  # Save conditions (thresholds) for each scenario    \n",
    "        return top_thresholds\n",
    "            \n",
    "tree = clf.tree_\n",
    "feature_names = X_train_bos.columns\n",
    "best_thresholds = get_positive_paths(tree, feature_names)\n",
    "thresholds = [thres[best_feature] for thres in best_thresholds if best_feature in thres]\n",
    "thresholds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "556916bd-3127-4b3d-aacc-52dbb708b952",
   "metadata": {},
   "outputs": [],
   "source": [
    "def discretize_and_sample(X_train, feature, thresholds, total_samples, num_bins):\n",
    "    feature_values = X_train[feature]  # Extract the column of interest\n",
    "    min_val, max_val = feature_values.min(), feature_values.max()\n",
    "\n",
    "    # Ensure the values are within the valid range of bins (clip out-of-range values)\n",
    "    feature_values = feature_values.clip(lower=min_val, upper=max_val)\n",
    "\n",
    "    # Dynamically calculate bin edges based on the feature values range\n",
    "    bin_edges = np.linspace(min_val, max_val, num_bins + 1)\n",
    "    bin_counts, bin_labels = pd.cut(feature_values, bins=bin_edges, labels=False, retbins=True)\n",
    "\n",
    "    selected_bins_by_threshold = []  # Combined threshold list\n",
    "    for thresh in thresholds:  # Iterate over age thresholds\n",
    "        selected_bins = []  # New bin list for that respective threshold\n",
    "        for bin_idx in range(num_bins):  # For each bin\n",
    "            bin_lower = bin_edges[bin_idx]  # Get lower bin threshold at bin_idx\n",
    "            bin_upper = bin_edges[bin_idx + 1]  # Get upper bin threshold at bin_idx\n",
    "            if thresh[0] <= bin_lower and thresh[1] >= bin_upper:  # If bin meets threshold requirements\n",
    "                selected_bins.append(bin_idx)\n",
    "        selected_bins_by_threshold.append(selected_bins)  # Add threshold bin list to combined threshold list\n",
    "    sampled_indices = set()\n",
    "\n",
    "    for threshold_idx, selected_bins in enumerate(selected_bins_by_threshold):\n",
    "        bin_freqs = feature_values.groupby(bin_counts).size()  # Calculate bin frequency of each bin for this threshold\n",
    "        \n",
    "        # Reindex to ensure all bins are accounted for, including those with zero frequency\n",
    "        bin_freqs = bin_freqs.reindex(range(num_bins), fill_value=0)\n",
    "        bin_priority = sorted([(bin_idx, bin_freqs[bin_idx])  # Sort by frequency of bin, decreasing\n",
    "                           for bin_idx in selected_bins], \n",
    "                          key=lambda x: -x[1]) \n",
    "        \n",
    "        for bin_idx, _ in bin_priority:  # Enumerate over the bins ordered by frequency for this threshold\n",
    "            bin_indices = feature_values[bin_counts == bin_idx].index  # Get the indices\n",
    "            needed = total_samples - len(sampled_indices)  # Get however many values are still needed to grab\n",
    "            \n",
    "            for idx in bin_indices:  # Iterate over indices\n",
    "                if len(sampled_indices) < total_samples:\n",
    "                    sampled_indices.add(idx)\n",
    "                else:\n",
    "                    break\n",
    "            if len(sampled_indices) >= total_samples:\n",
    "                break\n",
    "\n",
    "        if len(sampled_indices) >= total_samples:  # End early if total samples needed is met\n",
    "            break\n",
    "\n",
    "    #print(\"Selected bins:\", selected_bins_by_threshold)\n",
    "    #print(\"Sampled indices:\", sampled_indices)\n",
    "            \n",
    "    return list(sampled_indices), bin_edges\n",
    "\n",
    "boundary_indices_bos, bin_edges = discretize_and_sample(X_train_bos, best_feature, thresholds, total_samples=int(0.1 * len(X_train_bos)), num_bins=32)\n",
    "boundary_indices_bos_heau, bin_edges = discretize_and_sample(X_train_bos, best_feature, thresholds, total_samples=int(0.1 * len(X_train_bos)), num_bins=7)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "9f1831f5-10c8-4547-8b2f-c49300b890cb",
   "metadata": {},
   "outputs": [],
   "source": [
    "uncertain_radius_ins = 0.1*(y_train_ins.max() - y_train_ins.min())\n",
    "uncertain_radius_mpg = 0.1*(y_train_mpg.max() - y_train_mpg.min())\n",
    "uncertain_radius_bos = 0.1*(y_train_bos.max() - y_train_bos.min())\n",
    "uncertain_radii = [uncertain_radius_ins, uncertain_radius_mpg, uncertain_radius_bos, uncertain_radius_bos]\n",
    "\n",
    "uncertain_percentage = 0.1\n",
    "uncertain_num_ins = int(uncertain_percentage*len(y_train_ins))\n",
    "uncertain_num_mpg = int(uncertain_percentage*len(y_train_mpg))\n",
    "uncertain_num_bos = int(uncertain_percentage*len(y_train_bos))\n",
    "uncertain_numbers = [uncertain_num_ins, uncertain_num_mpg, uncertain_num_bos, uncertain_num_bos]\n",
    "\n",
    "dataset_sizes = [len(y_train_ins), len(y_train_mpg), len(y_train_bos), len(y_train_bos)]\n",
    "dataset_names = [\"Insurance\", \"MPG\", \"BOS\", \"BOS\"]\n",
    "\n",
    "dataset_dct = {}\n",
    "dataset_dct[\"Insurance\"] = [X_train_ins, X_test_ins, y_train_ins, y_test_ins]\n",
    "dataset_dct[\"MPG\"] = [X_train_mpg, X_test_mpg, y_train_mpg, y_test_mpg]\n",
    "dataset_dct[\"BOS\"] = [X_train_bos, X_test_bos, y_train_bos, y_test_bos]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "e2341b98-14ca-4dc0-99bf-94c05288b06d",
   "metadata": {},
   "outputs": [],
   "source": [
    "boundary_indices_lst.append(boundary_indices_bos)\n",
    "boundary_indices_lst.append(boundary_indices_bos_heau)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "4b2c70be-d7fd-4904-914d-936885bc213d",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Finding radius for Insurance:   0%|          | 0/500 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "11a3754d6f004c2f9645161dd6f6f6e9",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Finding radius for MPG:   0%|          | 0/500 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "0df939f505f04f228ad365cf8c8bc4ee",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Finding radius for BOS:   0%|          | 0/500 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "374a278b1a9d43969c419141a4d436aa",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Finding radius for BOS:   0%|          | 0/500 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Normalized robustness score for Insurance dataset is 0.5000\n",
      "Normalized robustness score for MPG dataset is 0.6481\n",
      "Normalized robustness score for BOS dataset is 0.6868\n",
      "Normalized robustness score for BOS dataset is 0.6868\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'Insurance': 0.5, 'MPG': 0.6480966263275809, 'BOS': 0.6867790286748094}"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def robustness_score_normalization(uncertain_numbers, uncertain_radii, dataset_sizes, boundary_indices_lst, dataset_names, dataset_dct):\n",
    "    robustness_radii_10 = [] #find robustness radius that grants radii robustness ratio of 0.10 or more \n",
    "                             #(alt. use 0.5 instead {depending on closeness, this ratio may need to be larger})\n",
    "\n",
    "    for i in range(0, len(dataset_names)):\n",
    "        uncertain_number = uncertain_numbers[i]\n",
    "        uncertain_radius = uncertain_radii[i]\n",
    "        boundary_indices = boundary_indices_lst[i]\n",
    "        X_train, X_test, y_train, y_test = dataset_dct[dataset_names[i]]\n",
    "        \n",
    "        robustness_radius=1\n",
    "        radius_increment = 3\n",
    "\n",
    "        robustness_ratio = compute_robustness_ratio_sensitive_label_error(X_train, y_train, X_test, y_test, \n",
    "                                                                    uncertain_num=uncertain_number,\n",
    "                                                                    boundary_indices=boundary_indices,\n",
    "                                                                    uncertain_radius=uncertain_radius, \n",
    "                                                                    robustness_radius=robustness_radius,\n",
    "                                                                    interval=False)\n",
    "        \n",
    "        with tqdm(total=500, desc=f\"Finding radius for {dataset_names[i]}\", leave=False) as pbar:\n",
    "            while robustness_ratio < 0.5:\n",
    "                robustness_radius += radius_increment\n",
    "                robustness_ratio = compute_robustness_ratio_sensitive_label_error(X_train, y_train, X_test, y_test, \n",
    "                                                                    uncertain_num=uncertain_number,\n",
    "                                                                    boundary_indices=boundary_indices,\n",
    "                                                                    uncertain_radius=uncertain_radius, \n",
    "                                                                    robustness_radius=robustness_radius,\n",
    "                                                                    interval=False)\n",
    "                pbar.update(radius_increment)\n",
    "        \n",
    "        robustness_radii_10.append(robustness_radius)\n",
    "\n",
    "    results = {}\n",
    "    for i, dataset_name in enumerate(dataset_names):\n",
    "        normalized_radius = (1 - (robustness_radii_10[i]/max(robustness_radii_10)))\n",
    "        normalized_size = (dataset_sizes[i]/max(dataset_sizes))\n",
    "        robustness_score = 0.5*normalized_radius + 0.5*normalized_size        \n",
    "        print(f\"Normalized robustness score for {dataset_name} dataset is {robustness_score:.4f}\")\n",
    "        results[dataset_name] = robustness_score\n",
    "    return results\n",
    "\n",
    "robustness_score_normalization(uncertain_numbers, uncertain_radii, dataset_sizes, boundary_indices_lst, dataset_names, dataset_dct)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "be4f1320-227a-461a-bdc2-906ec30e01eb",
   "metadata": {},
   "source": [
    "Normalized robustness score for Insurance dataset is 0.5000\n",
    "\n",
    "Normalized robustness score for MPG dataset is 0.6474\n",
    "\n",
    "{'Insurance': 0.5, 'MPG': 0.6473962077641984}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "25677a9a-a6f7-49b2-a832-24429737a828",
   "metadata": {},
   "outputs": [],
   "source": [
    "robustness_ratio = compute_robustness_ratio_sensitive_label_error(X_train_mpg, y_train_mpg, X_test_mpg, y_test_mpg, \n",
    "                                                                    uncertain_num=uncertain_num_mpg,\n",
    "                                                                    boundary_indices=boundary_indices_mpg,\n",
    "                                                                    uncertain_radius=uncertain_radius_mpg, \n",
    "                                                                    robustness_radius=1, \n",
    "                                                                    interval=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "24d54fed-5831-4c3d-9710-6e3155d0aa06",
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset_sizes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "25d226fb-13d5-42ac-ab7c-452d0e14c260",
   "metadata": {},
   "outputs": [],
   "source": [
    "lst = []\n",
    "lst.append(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "428fb120-796f-44ef-af1a-d8733e2b8e04",
   "metadata": {},
   "outputs": [],
   "source": [
    "lst"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6315e43c-6724-4d48-9b37-6a0b49e9e8af",
   "metadata": {},
   "outputs": [],
   "source": [
    "max(lst)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "30d88461-19f5-4bad-8290-efa4a69478d9",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
